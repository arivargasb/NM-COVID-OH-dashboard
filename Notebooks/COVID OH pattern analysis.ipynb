{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve available transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transcript_list_20_04_15 = YouTubeTranscriptApi.list_transcripts('wCaDb78vOtY') # apr 15 --> 1\n",
    "transcript_list_20_04_20 = YouTubeTranscriptApi.list_transcripts('9uh2Zq7rpWI') # apr 20 --> 2\n",
    "transcript_list_20_04_22 = YouTubeTranscriptApi.list_transcripts('67ohvRQJclo') # apr 22 --> 3\n",
    "transcript_list_20_04_27 = YouTubeTranscriptApi.list_transcripts('xb2GM1EiZco') # apr 27 --> 4\n",
    "transcript_list_20_05_04 = YouTubeTranscriptApi.list_transcripts('CPErAd9ZEMY') # may 4 --> 5\n",
    "transcript_list_20_05_11 = YouTubeTranscriptApi.list_transcripts('txP6dO0i_4I') # may 11 --> 6\n",
    "transcript_list_20_05_18 = YouTubeTranscriptApi.list_transcripts('O_wMUvFxqh0') # may 18 --> 7\n",
    "transcript_list_20_06_01 = YouTubeTranscriptApi.list_transcripts('mLg8txbrh18') # jun 1 --> 8\n",
    "transcript_list_20_06_08 = YouTubeTranscriptApi.list_transcripts('AdMHXOIM3wc') # jun 8 --> 9\n",
    "transcript_list_20_06_15 = YouTubeTranscriptApi.list_transcripts('_4fsBL5R4rA') # jun 15 -->10\n",
    "transcript_list_20_06_29 = YouTubeTranscriptApi.list_transcripts('s9UBbpJ5_yY') # jun 29 --> 11\n",
    "transcript_list_20_07_13 = YouTubeTranscriptApi.list_transcripts('FBuHAY2Q6iE') # jul 13 --> 12\n",
    "transcript_list_20_07_20 = YouTubeTranscriptApi.list_transcripts('Pvv_5u-JgkU') # jul 20 --> 13\n",
    "transcript_list_20_07_27 = YouTubeTranscriptApi.list_transcripts('6KTqHoZ5yF0') # jul 27 --> 14\n",
    "transcript_list_20_08_03 = YouTubeTranscriptApi.list_transcripts('iBx5-XE2jWo') # aug 3 --> 15\n",
    "transcript_list_20_08_10 = YouTubeTranscriptApi.list_transcripts('N9sFlng3z6M') # aug 10 --> 16\n",
    "transcript_list_20_08_17 = YouTubeTranscriptApi.list_transcripts('727l1vOlXhA') # aug 17 --> 17\n",
    "transcript_list_20_08_24 = YouTubeTranscriptApi.list_transcripts('B9EV7x1Vv-c') # aug 24 --> 18\n",
    "transcript_list_20_08_31 = YouTubeTranscriptApi.list_transcripts('4uGpiZSZ0Xk') # aug 31 --> 19\n",
    "transcript_list_20_09_14 = YouTubeTranscriptApi.list_transcripts('3fOkP7nNajw') # sep 14 --> 20\n",
    "transcript_list_20_09_21 = YouTubeTranscriptApi.list_transcripts('w2-SNHvH-QM') # sep 21 --> 21\n",
    "transcript_list_20_09_28 = YouTubeTranscriptApi.list_transcripts('21FeT1Z_ntg') # sep 28 --> 22\n",
    "transcript_list_20_10_05 = YouTubeTranscriptApi.list_transcripts('TpgQnORqnpc') # oct 5 --> 23\n",
    "transcript_list_20_10_12 = YouTubeTranscriptApi.list_transcripts('SXgs5iXN2jE') # oct 12 --> 24\n",
    "transcript_list_20_10_26 = YouTubeTranscriptApi.list_transcripts('fIW_6SrhhJo') # oct 26 --> 25\n",
    "transcript_list_20_11_02 = YouTubeTranscriptApi.list_transcripts('if9ybN6SzdU') # nov 2 --> 26\n",
    "transcript_list_20_11_09 = YouTubeTranscriptApi.list_transcripts('rD6xN51EvLI') # nov 9 --> 27\n",
    "transcript_list_20_11_16 = YouTubeTranscriptApi.list_transcripts('aj4wCjhU25E') # nov 16  --> 28\n",
    "transcript_list_20_11_30 = YouTubeTranscriptApi.list_transcripts('Tl9xdBbEWtU') # nov 30 --> 29\n",
    "transcript_list_20_12_07 = YouTubeTranscriptApi.list_transcripts('Kp5--a_Or40') # dec 7 --> 30\n",
    "transcript_list_20_12_14 = YouTubeTranscriptApi.list_transcripts('O5Y1tZtcA-I') # dec 14 -->31\n",
    "transcript_list_20_12_21 = YouTubeTranscriptApi.list_transcripts('G5yQOSZs228') # dec 21 --> 32\n",
    "transcript_list_21_01_04 = YouTubeTranscriptApi.list_transcripts('XwG1WVErygE') # jan 4 --> 33\n",
    "transcript_list_21_01_11 = YouTubeTranscriptApi.list_transcripts('yvehrgqsdVg') # jan 11 --> 34\n",
    "transcript_list_21_01_25 = YouTubeTranscriptApi.list_transcripts('eoxCOMH2e6U') # jan 25 --> 35\n",
    "transcript_list_21_02_01 = YouTubeTranscriptApi.list_transcripts('k3WnSt7Q8u4') # feb 1 --> 36\n",
    "transcript_list_21_02_08 = YouTubeTranscriptApi.list_transcripts('MgU9O8tWQjQ') # feb 8 --> 37\n",
    "transcript_list_21_02_15 = YouTubeTranscriptApi.list_transcripts('olwBArtWj30') # feb 15 --> 38\n",
    "transcript_list_21_02_22 = YouTubeTranscriptApi.list_transcripts('kSki_k4eYYk') # feb 22 --> 39\n",
    "transcript_list_21_03_08 = YouTubeTranscriptApi.list_transcripts('6j4ksYnvhv4') # mar 8 --> 40\n",
    "transcript_list_21_03_15 = YouTubeTranscriptApi.list_transcripts('Ph6aHWAVe3g') # mar 15 --> 41\n",
    "transcript_list_21_03_22 = YouTubeTranscriptApi.list_transcripts('H1KeNsiVALc') # mar 22 --> 42\n",
    "transcript_list_21_03_29 = YouTubeTranscriptApi.list_transcripts('MQ77n9-QikQ') # mar 29 --> 43\n",
    "transcript_list_21_04_05 = YouTubeTranscriptApi.list_transcripts('wvSDW0T1cNk') # apr 5 --> 44\n",
    "transcript_list_21_04_12 = YouTubeTranscriptApi.list_transcripts('4KhsoymQ1zM') # apr 12 --> 45\n",
    "transcript_list_21_04_19 = YouTubeTranscriptApi.list_transcripts('LvKtcDJBySk') # apr 19 --> 46\n",
    "transcript_list_21_04_26 = YouTubeTranscriptApi.list_transcripts('alcK8CuBkM4') # apr 26 --> 47\n",
    "transcript_list_21_05_10 = YouTubeTranscriptApi.list_transcripts('7PRCqtNmGBc') # may 10 --> 48\n",
    "\n",
    "transcript_list=zip(\n",
    "                   transcript_list_20_04_15, transcript_list_20_04_20, transcript_list_20_04_22, \n",
    "                    transcript_list_20_04_27, transcript_list_20_05_04, transcript_list_20_05_11,\n",
    "                    transcript_list_20_05_18, transcript_list_20_06_01, transcript_list_20_06_08, transcript_list_20_06_15, \n",
    "                    transcript_list_20_06_29, transcript_list_20_07_13, transcript_list_20_07_20, \n",
    "                    transcript_list_20_07_27, transcript_list_20_08_03, transcript_list_20_08_10, \n",
    "                    transcript_list_20_08_17, transcript_list_20_08_24 , transcript_list_20_08_31, \n",
    "                    transcript_list_20_09_14, transcript_list_20_09_21,  transcript_list_20_09_28,\n",
    "                    transcript_list_20_10_05, transcript_list_20_10_12, transcript_list_20_10_26,\n",
    "                    transcript_list_20_11_02,\n",
    "                    transcript_list_20_11_09, transcript_list_20_11_16, transcript_list_20_11_30,\n",
    "                    transcript_list_20_12_07, transcript_list_20_12_14, transcript_list_20_12_21,\n",
    "                    transcript_list_21_01_04, transcript_list_21_01_11, transcript_list_21_01_25,\n",
    "                    transcript_list_21_02_01, transcript_list_21_02_08, transcript_list_21_02_15,\n",
    "                    transcript_list_21_02_22, transcript_list_21_03_08, transcript_list_21_03_15,\n",
    "                    transcript_list_21_03_22, transcript_list_21_03_29, transcript_list_21_04_05,\n",
    "                    transcript_list_21_04_12, transcript_list_21_04_19, transcript_list_21_04_26, transcript_list_21_05_10)\n",
    "\n",
    "\n",
    "# # iterate over all available transcripts\n",
    "for transcript in transcript_list:\n",
    "    for i in range(48):\n",
    "        # the Transcript object provides metadata properties\n",
    "        print(transcript[i].video_id)\n",
    "#             transcript[i].language,\n",
    "#             transcript[i].language_code,\n",
    "#     #         # whether it has been manually created or generated by YouTube\n",
    "#             transcript[i].is_generated,\n",
    "#     #         # whether this transcript can be translated or not\n",
    "#             transcript[i].is_translatable,\n",
    "#     #         # a list of languages the transcript can be translated to\n",
    "#             transcript[i].translation_languages,\n",
    "#     )\n",
    "\n",
    "    #     # fetch the actual transcript data\n",
    "#         print(transcript[i].fetch())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list = [[],[]]\n",
    "\n",
    "transcript_list=ziptranscript_list=zip(\n",
    "                   transcript_list_20_04_15, transcript_list_20_04_20, transcript_list_20_04_22, \n",
    "                    transcript_list_20_04_27, transcript_list_20_05_04, transcript_list_20_05_11,\n",
    "                    transcript_list_20_05_18, transcript_list_20_06_01, transcript_list_20_06_08, transcript_list_20_06_15, \n",
    "                    transcript_list_20_06_29, transcript_list_20_07_13, transcript_list_20_07_20, \n",
    "                    transcript_list_20_07_27, transcript_list_20_08_03, transcript_list_20_08_10, \n",
    "                    transcript_list_20_08_17, transcript_list_20_08_24 , transcript_list_20_08_31, \n",
    "                    transcript_list_20_09_14, transcript_list_20_09_21,  transcript_list_20_09_28,\n",
    "                    transcript_list_20_10_05, transcript_list_20_10_12, transcript_list_20_10_26,\n",
    "                    transcript_list_20_11_02,\n",
    "                    transcript_list_20_11_09, transcript_list_20_11_16, transcript_list_20_11_30,\n",
    "                    transcript_list_20_12_07, transcript_list_20_12_14, transcript_list_20_12_21,\n",
    "                    transcript_list_21_01_04, transcript_list_21_01_11, transcript_list_21_01_25,\n",
    "                    transcript_list_21_02_01, transcript_list_21_02_08, transcript_list_21_02_15,\n",
    "                    transcript_list_21_02_22, transcript_list_21_03_08, transcript_list_21_03_15,\n",
    "                    transcript_list_21_03_22, transcript_list_21_03_29, transcript_list_21_04_05,\n",
    "                    transcript_list_21_04_12, transcript_list_21_04_19, transcript_list_21_04_26, transcript_list_21_05_10)\n",
    "\n",
    "for transcript in transcript_list:\n",
    "    for i in range(48):\n",
    "        trans=transcript[i].fetch()\n",
    "#         trans = transcript[i].fetch()\n",
    "#         print(trans)\n",
    "        temp=[]\n",
    "        for x in range(len(trans)):\n",
    "            temp.append(trans[x]['text'])\n",
    "        trans_list[0].append(i)\n",
    "        joined = \" \".join(temp)\n",
    "#         print(joined)\n",
    "        trans_list[1].append(joined)\n",
    "\n",
    "\n",
    "#         trans = transcript[i].fetch()\n",
    "#         for x in range(len(trans)):\n",
    "#             trans_list[0].append(i)\n",
    "#             trans_list[1].append(trans[x]['text'])\n",
    "#             print(trans[x]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(trans_list[1])\n",
    "# trans_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COVID19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for t in range(len(trans_list[0])):\n",
    "#     if trans_list[0][t]==0:\n",
    "#         print(trans_list[1][t])\n",
    "\n",
    "trans_list_temp=[]\n",
    "for t in range(len(trans_list[1])):\n",
    "    trans_list_temp.append(trans_list[1][t])\n",
    "\n",
    "session_list_temp=[]\n",
    "for t in range(len(trans_list[0])):\n",
    "    session_list_temp.append(trans_list[0][t])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(session_list_temp)\n",
    "trans_list_temp\n",
    "trans_list_clean = [t.replace('kobe', 'covid').\n",
    "                    replace('kovat 19', 'covid'). \n",
    "                    replace('co vid 19', 'covid').\n",
    "                    replace('co bid 19', 'covid').\n",
    "                    replace('coab it 19', 'covid').\n",
    "                    replace('coded 19', 'covid').\n",
    "                    replace('covet', 'covid').\n",
    "                    replace('koban 19', 'covid').\n",
    "                    replace('covin 19', 'covid').\n",
    "                    replace('cobit 19', 'covid').\n",
    "                    replace('kabocha 19', 'covid').\n",
    "                    replace('cova', 'covid').\n",
    "                    replace('koban', 'covid').\n",
    "                    replace('kobi', 'covid').\n",
    "                    replace('sarskoby', 'covid').\n",
    "                    replace('collee', 'covid').\n",
    "                    replace('covey', 'covid').\n",
    "                    replace('copy 19', 'covid').\n",
    "                    replace('cop2', 'covid').\n",
    "                    replace('sarcobi2', 'covid').\n",
    "                    replace('kobe 2', 'covid').\n",
    "                    replace('sarcoby', 'covid').\n",
    "                    replace('sarcobytes', 'covid').\n",
    "                    replace('sarcoby2', 'covid').\n",
    "                    replace('malcolm', 'mild-covid').\n",
    "                    replace('moderate copy', 'moderate-covid').                    \n",
    "                    replace('copy patient', 'covid patient').\n",
    "                    replace('copy patients', 'covid patients').\n",
    "                    replace('severe copy', 'severe-covid').\n",
    "                    replace('severe copies', 'severe-covid').                    \n",
    "                    replace('mild copy', 'mild-covid').\n",
    "                    replace('non-critical copy', 'non-critical-covid').\n",
    "                    replace('suspicion of copy', 'suspicion of covid').\n",
    "                    replace('copies', 'covid').\n",
    "                    replace('copic', 'covid').                    \n",
    "                    replace('copen', 'covid') for t in trans_list_temp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('arts', 'ards').\n",
    "                    replace('air d s', 'ards'). \n",
    "                    replace('dards', 'ards') for t in trans_list_clean]\n",
    "\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remdesivir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('rendezvous', 'remdesivir'). \n",
    "                    replace('render severe', 'remdesivir'). \n",
    "                    replace('remdesevere', 'remdesivir'). \n",
    "                    replace('rendezvir', 'remdesivir').\n",
    "                    replace('rendestivir', 'remdesivir'). \n",
    "                    replace('rendessive', 'remdesivir'). \n",
    "                    replace('remdessevier', 'remdesivir') for t in trans_list_clean]\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lopinavir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('lopina beer', 'lopinavir'). \n",
    "                    replace('lopinabe', 'lopinavir') for t in trans_list_clean]\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('wa show', 'who') for t in trans_list_clean]\n",
    "# trans_list_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tocilizumab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('toscilisumab', 'tocilizumab'). \n",
    "                    replace('tosinizuma', 'tocilizumab') for t in trans_list_clean]\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### viral load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('buyer load', 'viral-load').\n",
    "                    replace('vital load', 'viral-load').\n",
    "                    replace('viral loads', 'viral-load').\n",
    "                    replace('wire load', 'viral-load').\n",
    "                    replace('hybrid load', 'high-viral-load').\n",
    "                    replace('viral loading', 'viral-load in').\n",
    "                    replace('viral shedding', 'viral-shedding').                    \n",
    "                    replace('viral law', 'viral-load') for t in trans_list_clean]\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IL-17 and IL-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('il-17', 'il17').\n",
    "                    replace('il-16', 'il16').\n",
    "                    replace('io17', 'il17') for t in trans_list_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### skull base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('school-based', 'skull base') for t in trans_list_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### astrazeneca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('astra seneca', 'astrazeneca').\n",
    "                    replace('astral seneca', 'astrazeneca').\n",
    "                    replace('astra zeneca', 'astrazeneca') for t in trans_list_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### novavax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('novafax', 'novavax').\n",
    "                    replace('nova fax', 'novavax').                    \n",
    "                    replace('nova box', 'novavax').                    \n",
    "                    replace('novabox', 'novavax').                    \n",
    "                    replace('novabacks', 'novavax').\n",
    "                    replace('novaback', 'novavax').                    \n",
    "                    replace('nova back', 'novavax').                                        \n",
    "                    replace('nova backs', 'novavax').\n",
    "                    replace('nova vax', 'novavax') for t in trans_list_clean]\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sinovac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('sino back', 'sinovac').\n",
    "                    replace('cynobac', 'sinovac').                    \n",
    "                    replace('sino farm', 'sinopharm').                    \n",
    "                    replace('sinofarm', 'sinopharm').                               \n",
    "                    replace('sino pharm', 'sinopharm').\n",
    "                    replace('cyanobac', 'sinovac') for t in trans_list_clean]\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### johnson&johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('johnson and johnson', 'johnson&johnson').\n",
    "                    replace('janssen', 'johnson&johnson').\n",
    "                    replace('maxine', 'vaccine') for t in trans_list_clean]\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### moderna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('modern', 'moderna').\n",
    "                    replace('modernize', 'moderna').\n",
    "                    replace(\"moderna's\", 'moderna'). \n",
    "                    replace(\"modernity\", 'moderna').\n",
    "                    replace(\"modernaa\", 'moderna').                    \n",
    "                    replace('pfeister', 'pfizer') for t in trans_list_clean]\n",
    "# trans_list_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trans_list_clean)\n",
    "len(session_list_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list_clean = [t.replace('monoclonal antibodies', 'monoclonal-antibody ').\n",
    "                    replace('monoclonal ', 'monoclonal-antibody ').\n",
    "                    replace('monoclonals ', 'monoclonal-antibody ').\n",
    "                    replace('monoclonal antibody', 'monoclonal-antibody ').\n",
    "                    replace('antivirals', 'antiviral').\n",
    "                    replace('landsat', 'lancet').                    \n",
    "                    replace('t cell ', 't-cell').\n",
    "                    replace('t cells', 't-cell').\n",
    "                    replace('blood thiner', 'blood-thiner').                    \n",
    "                    replace('vitamin d', 'vitamin-d').\n",
    "                    replace('vitamin c', 'vitamin-c').     \n",
    "                    replace('eli lilly', 'eli-lilly').                    \n",
    "                    replace(\"pre existing condition\", 'pre-existing-condition'). \n",
    "                    replace(\"shortness of breath\", 'shortness-of-breath').                     \n",
    "                    replace(\"dry cough\", 'dry-cough').                                         \n",
    "                    replace(\"sore throat\", 'sore-throat').     \n",
    "                    replace(\"chest pain\", 'chest-pain').                      \n",
    "                    replace(\"chest pressure\", 'chest-pressure').                                          \n",
    "                    replace(\"joint pain\", 'joint-pain').                                                              \n",
    "                    replace(\"inflammatory markers\", 'inflammatory-markers').\n",
    "                    replace(\"inflammation\", 'inflammatory-markers').\n",
    "                    replace(\"inflammatory\", 'inflammatory-markers').                    \n",
    "                    replace(\"lock down\", 'lock-down').    \n",
    "                    replace(\"lockdown\", 'lock-down').    \n",
    "                    replace(\"personal protective equipment\", 'ppe').                                                                                  \n",
    "                    replace(\"personal equipment\", 'ppe').                                                                                                      \n",
    "                    replace(\"protective equipment\", 'ppe').    \n",
    "                    replace(\"antibodies\", 'antibody').                     \n",
    "                    replace(\"antibody test\", 'antibody-test').   \n",
    "                    replace(\"antigen test\", 'antigen-test').                       \n",
    "                    replace(\"case fatality\", 'case-fatality').                                           \n",
    "                    replace(\"case fatality ratio\", 'case-fatality').                                                               \n",
    "                    replace(\"contact tracing\", 'contact-tracing').                                                                                   \n",
    "                    replace(\"blood test\", 'blood-test').\n",
    "                    replace(\"excess mortality\", 'excess-mortality').   \n",
    "                    replace(\"false negative\", 'false-negative').   \n",
    "                    replace(\"false positive\", 'false-positive').   \n",
    "                    replace(\"excess mortality\", 'excess-mortality').   \n",
    "                    replace(\"flatten the curve\", 'flatten-the-curve').   \n",
    "                    replace(\"transmission rate\", 'transmission-rate').   \n",
    "                    replace(\"immunity passport\", 'vaccine-passport').\n",
    "                    replace(\"vaccine passport\", 'vaccine-passport').                    \n",
    "                    replace(\"infection fatality\", 'infection-fatality').                       \n",
    "                    replace(\"index case\", 'index-case').                       \n",
    "                    replace(\"lamp test\", 'lamp-test').                       \n",
    "                    replace(\"infection fatality\", 'test').                       \n",
    "                    replace(\"tested\", 'test').                       \n",
    "                    replace(\"testinhg\", 'testing').                       \n",
    "                    replace(\"lateral flow test\", 'lateral-flow-test').                                           \n",
    "                    replace(\"mass testing\", 'mass-testing').                                           \n",
    "                    replace(\"point of care\", 'point-of-care').                                           \n",
    "                    replace(\"pooled testing\", 'pooled-testing').                                           \n",
    "                    replace(\"reproduction number\", 'reproduction-number').                                           \n",
    "                    replace(\"primary case\", 'primary-case').                                                                                                    \n",
    "                    replace(\"rapid test\", 'rapid-test').                                           \n",
    "                    replace(\"saliva test\", 'saliva-test').                                                                                                      \n",
    "                    replace(\"s gene\", 's-gene').\n",
    "                    replace(\"self sampling\", 'self-sampling').\n",
    "                    replace(\"self test\", 'self-test').\n",
    "                    replace(\"s gene\", 's-gene').\n",
    "                    replace(\"sterilizing vaccine\", 'sterilizing-vaccine').\n",
    "                    replace(\"sterilizing immunity\", 'sterilizing-immunity').                    \n",
    "                    replace(\"vaccine rollout\", 'vaccine-rollout').\n",
    "                    replace(\"vaccine coverage\", 'vaccine-coverage').                    \n",
    "                    replace(\"vaccine take up\", 'vaccine-takeup').                                        \n",
    "                    replace(\"vaccine takeup\", 'vaccine-takeup').                                                            \n",
    "                    replace(\"vaccine candidate\", 'vaccine-candidate').   \n",
    "                    replace(\"vaccines\", 'vaccine').                       \n",
    "                    replace(\"vaccinated\", 'vaccine').                                           \n",
    "                    replace(\"vaccination\", 'vaccine').        \n",
    "                    replace(\"vaccinating\", 'vaccine').                            \n",
    "                    replace(\"vaccinate\", 'vaccine').                                                               \n",
    "                    replace(\"take up\", 'takeup').                                                                                                    \n",
    "                    replace(\"take-up\", 'takeup').                                                                                                                        \n",
    "                    replace(\"viral vector\", 'viral-vector').\n",
    "                    replace(\"viral vectored\", 'viral-vector').   \n",
    "                    replace(\"vectored-vaccine\", 'viral-vector').                           \n",
    "                    replace(\"vectored-vaccine\", 'viral-vector').       \n",
    "                    replace(\"clinical trials\", 'clinical-trial').                           \n",
    "                    replace(\"clinical trial\", 'clinical-trial').                                               \n",
    "                    replace(\"phase 1\", 'phase-1').                    \n",
    "                    replace(\"phase 2\", 'phase-2').                    \n",
    "                    replace(\"phase 3\", 'phase-3').                    \n",
    "                    replace(\"phase 4\", 'phase-4').          \n",
    "                    replace(\"anticoagulation\", 'anticoagulant').          \n",
    "                    replace(\"randomized controlled trial\", 'randomized-controlled-trial').                        \n",
    "                    replace(\"randomize control trial\", 'randomized-controlled-trial').                                            \n",
    "                    replace(\"randomized control trial\", 'randomized-controlled-trial').                                                                \n",
    "                    replace(\"randomize controlled trial\", 'randomized-controlled-trial'). \n",
    "                    replace(\"randomize-controlled-trials\", 'randomized-controlled-trial').                     \n",
    "                    replace(\"randomized \", 'randomized-controlled-trial ').                                         \n",
    "                    replace(\" trials \", ' clinical-trial ').                                         \n",
    "                    replace(\" trial \", ' clinical-trial ').                     \n",
    "                    replace(\"rct\", 'randomized-controlled-trial').                     \n",
    "                    replace(\"peer review\", 'peer-review').                                                                \n",
    "                    replace(\"new england journal\", 'new-england-journal').                                                                                    \n",
    "                    replace(\"nature magazine\", 'nature-magazine').                                                                \n",
    "                    replace(\"immune response\", 'immune-response').   \n",
    "                    replace(\"immune system\", 'immune-system').   \n",
    "                    replace(\"active component\", 'active-component').   \n",
    "                    replace(\"odds ratio\", 'odds-ratio').                       \n",
    "                    replace(\"attenuated vaccine\", 'attenuated-vaccine'). \n",
    "                    replace(\"attenuated virus\", 'attenuated-virus').                     \n",
    "                    replace(\"b cell \", 'b-cell').                       \n",
    "                    replace(\"b cells\", 'b-cell').                                           \n",
    "                    replace(\"odds ratio\", 'odds-ratio').\n",
    "                    replace(\"booster dose\", 'booster-dose'). \n",
    "                    replace(\"dna based\", 'dna-based'). \n",
    "                    replace(\"cold chain\", 'cold-chain'). \n",
    "                    replace(\"ventilation\", 'ventilator').                     \n",
    "                    replace(\"disease modifying\", 'disease-modifying'). \n",
    "                    replace(\"dosing interval\", 'dosing-interval').                     \n",
    "                    replace(\"dose interval\", 'dosing-interval').        \n",
    "                    replace(\"innactivated virus\", 'innactivated-virus').                            \n",
    "                    replace(\"innactivated vaccine\", 'innactivated-virus').                                                \n",
    "                    replace(\"messenger rna\", 'mrna').                        \n",
    "                    replace(\"mrna vaccine\", 'mrna-vaccine').    \n",
    "                    replace(\"mrna vaccines\", 'mrna-vaccine').\n",
    "                    replace(\"herd immunity\", 'herd-immunity').                        \n",
    "                    replace(\"passive immunity\", 'passive-immunity').                                            \n",
    "                    replace(\"neutralizing antibodies\", 'neutralizing-antibody').                                                                \n",
    "                    replace(\"neutralizing antibody\", 'neutralizing-antibody').                                                                                    \n",
    "                    replace(\"second dose\", 'second-dose').                                                                                                        \n",
    "                    replace(\"first dose\", 'first-dose').                                                                                                                            \n",
    "                    replace(\"priority groups\", 'priority-groups').\n",
    "                    replace(\"priority group\", 'priority-groups').          \n",
    "                    replace(\"protein based\", 'protein-based').                              \n",
    "                    replace(\"priority group\", 'priority-groups').    \n",
    "                    replace(\"amplifying rna\", 'amplifying-rna').   \n",
    "                    replace(\"swab test\", 'swab-test').                       \n",
    "                    replace(\"super spreader\", 'superspreader').                                           \n",
    "                    replace(\"super spreading\", 'superspreader').  \n",
    "                    replace(\"social distance\", 'social-distance').                      \n",
    "                    replace(\"social distancing\", 'social-distance').    \n",
    "                    replace(\"wash your hands\", 'hand-washing').  \n",
    "                    replace(\"wash hands\", 'hand-washing').                      \n",
    "                    replace(\"wash your hands\", 'hand-washing').                                          \n",
    "                    replace(\"hand washing\", 'hand-washing').                                                              \n",
    "                    replace(\"n 95\", 'n95').                        \n",
    "                    replace(\"n-95\", 'n95'). \n",
    "                    replace(\"n95s\", 'n95'). \n",
    "                    replace(\"proning\", 'prone'). \n",
    "                    replace(\"smell\", 'anosmia').\n",
    "                    replace(\"difficulty breathing\", 'dyspnea').                    \n",
    "                    replace(\"shortness of breath\", 'dyspnea').                                        \n",
    "                    replace(\"breath\", 'dyspnea').                                                            \n",
    "                    replace(\"clot\", 'clotting').                                                                                \n",
    "                    replace(\"rash\", 'rashes').  \n",
    "                    replace(\"pain\", 'aches').                      \n",
    "                    replace(\"pains\", 'aches').                                          \n",
    "                    replace(\"ache\", 'aches').\n",
    "                    replace(\"epidemiologist\", 'epidemiology').\n",
    "                    replace(\"hygienic\", 'hygiene').                    \n",
    "                    replace(\"antigens\", 'antigen').    \n",
    "                    replace(\"antigenic\", 'antigen').\n",
    "                    replace(\"mutations\", 'mutation').                    \n",
    "                    replace(\"shedding\", 'viral-shedding').   \n",
    "                    replace(\"pathogens\", 'pathogen').   \n",
    "                    replace(\"antibiotics\", 'antibiotic').        \n",
    "                    replace(\"double blind\", 'double-blind').  \n",
    "                    replace(\"odds\", 'odds-ratio').                      \n",
    "                    replace(\"obese\", 'obesity').                                          \n",
    "                    replace(\"old age\", 'elderly').                                                              \n",
    "                    replace(\"hispanics\", 'hispanic').                                                                                 \n",
    "                    replace(\"blacks\", 'black').                                                                                                     \n",
    "                    replace(\"ethnic\", 'ethnicity').   \n",
    "                    replace(\"racial\", 'race').           \n",
    "                    replace(\"diabetic\", 'diabetes').     \n",
    "                    replace(\"hypertensive\", 'hypertense').            \n",
    "                    replace(\"native americans\", 'native-american').                     \n",
    "                    replace(\"native american\", 'native-american').                                                    \n",
    "                    replace(\"relative risk\", 'relative-risk') for t in trans_list_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import COVID lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lexicon= pd.read_csv('../data/raw/lexicon_covid.csv', sep =';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "lexicon.reset_index( inplace=True)\n",
    "lexicon=lexicon.rename(columns={'index':'word'})\n",
    "lexicon\n",
    "\n",
    "conditions = [\n",
    "    (lexicon['treatment'] == 1),\n",
    "    (lexicon['symptom'] == 1), \n",
    "    (lexicon['spread'] == 1), \n",
    "    (lexicon['biology'] == 1), \n",
    "    (lexicon['determinants'] == 1), \n",
    "    (lexicon['immunization'] == 1), \n",
    "    (lexicon['organizations'] == 1), \n",
    "    (lexicon['research'] == 1), \n",
    "    (lexicon['statistics'] == 1)    \n",
    "    ]\n",
    "\n",
    "values = ['treatment', 'symptom', 'spread', 'biology', 'determinants', \n",
    "          'immunization', 'organizations', 'research', 'statistics']\n",
    "\n",
    "lexicon['type'] = np.select(conditions, values)\n",
    "lexicon['type'].unique()\n",
    "lexicon\n",
    "\n",
    "lexicon=lexicon.drop(['treatment', 'symptom', 'spread', 'biology', \n",
    "              'determinants', 'immunization', 'organizations', \n",
    "              'research', 'statistics'], axis=1)\n",
    "lexicon\n",
    "vocabulary = lexicon['word'].tolist()\n",
    "len(vocabulary)\n",
    "# vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment-related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = (lexicon[\"type\"] == 'treatment')\n",
    "df_treatment = lexicon.loc[treatment]\n",
    "df_treatment\n",
    "\n",
    "vocab_treatment = df_treatment['word'].tolist()\n",
    "len(vocab_treatment)\n",
    "vocab_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "count_dict={}\n",
    "for v in vocab_treatment:\n",
    "    items=[]\n",
    "    x=1\n",
    "    while x<=len(trans_list_clean):\n",
    "        items.append(0)\n",
    "        x+=1\n",
    "    count_dict[v]=items\n",
    "#     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "count_dict\n",
    "\n",
    "df = pd.DataFrame(count_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trans_list_clean)):\n",
    "    words=trans_list_clean[i].split()\n",
    "    listemmed = stemmer.stem(trans_list_clean[i].lower())\n",
    "#     count=0\n",
    "#     filtered = []\n",
    "    for word in words:\n",
    "        word_stemmed = stemmer.stem(word.lower())\n",
    "        count=0\n",
    "        count = vocab_treatment.count(word.lower())\n",
    "        if count==1:\n",
    "            print(word.lower())\n",
    "            count_dict[word.lower()][i]+=1\n",
    "#         print(word_stemmed)\n",
    "#     if word_stemmed in vocabulary:\n",
    "#         filtered.append(word_stemmed)\n",
    "\n",
    "count_dict\n",
    "\n",
    "# Counter(words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "treatment = pd.DataFrame(count_dict)\n",
    "treatment\n",
    "treatment.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with zeros only\n",
    "treatment_filt=treatment.loc[(treatment.sum(axis=1) != 0), (treatment.sum(axis=0) != 0)]\n",
    "treatment_filt.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "treatment_filt=treatment_filt.drop(['antibody', 'cell', 'dose', 'doses', 'vitamin', 'eli-lilly', 'invasive',\n",
    "                    'drug', 'drugs', 'treatment', 'treatments'], axis = 1)\n",
    "\n",
    "treatment_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "treatment_filt_norm = min_max_scaling(treatment_filt)\n",
    "\n",
    "treatment_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_string = ''.join(trans_list_clean[0])\n",
    "joined_string[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "# tokens.remove('')        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame({'day': ['4/15/2020', '4/20/2020', '4/22/2020', \n",
    "                              '4/27/2020', '5/4/2020', '5/11/2020', '5/18/2020', '6/1/2020', '6/8/2020', \n",
    "                            '6/15/2020', '6/29/2020', '7/13/2020', '7/20/2020', '7/27/2020', \n",
    "                              '8/3/2020', '8/10/2020', '8/17/2020', '8/24/2020', '8/31/2020',\n",
    "                            '9/14/2020', '9/21/2020',  '9/28/2020', '10/5/2020', '10/20/2020',\n",
    "                             '10/26/2020', '11/2/2020', '11/9/2020',\n",
    "                             '11/16/2020', '11/30/2020', '12/7/2020', \n",
    "                              '12/14/2020', '12/21/2020', '1/4/2021',\n",
    "                             '1/11/2021', '1/25/2021', '2/1/2021', \n",
    "                              '2/8/2021', '2/15/2021', '2/22/2021',\n",
    "                             '3/8/2021', '3/15/2021', '3/22/2021', \n",
    "                              '3/29/2021', '4/5/2021', '4/12/2021',\n",
    "                             '4/19/2021', '4/26/2021', '5/10/2021']})\n",
    "\n",
    "treatment_filt_norm['day'] = dates\n",
    "treatment_filt_norm['day'] = pd.to_datetime(treatment_filt_norm['day'])\n",
    "treatment_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm.to_csv('../data/clean/test.csv')\n",
    "treatment_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_low = treatment_filt_norm.columns.values.tolist()\n",
    "columns_title=[]\n",
    "for c in columns_low:\n",
    "    x=c.title()\n",
    "    columns_title.append(x)\n",
    "treatment_filt_norm.reset_index(drop=True, inplace=True)\n",
    "treatment_filt_norm.columns\n",
    "columns=[ 'hydroxychloroquine',\n",
    "         'chloroquine',\n",
    "                   'prone',\n",
    "         'dexamethasone',\n",
    "          'anticoagulant',\n",
    "                   'remdesivir',\n",
    "                   'rna',\n",
    "          'prophylaxis',\n",
    "          'plasma',\n",
    "           'ventilator',\n",
    "          'interferon',\n",
    "          'corticosteroids',\n",
    "    'antiviral',\n",
    "          'azithromycin',\n",
    "          'favipiravir',\n",
    " 'monoclonal-antibody',\n",
    "          'antibiotic',\n",
    "        'ibuprofen',\n",
    "          'vitamin-d',\n",
    " 'tocilizumab',\n",
    " 'vitamin-c',\n",
    " 'zinc',\n",
    " 'regeneron',\n",
    "          'sarilumab',\n",
    " 'day']\n",
    "\n",
    "treatment_filt_norm.info()\n",
    "treatment_filt_norm = treatment_filt_norm[columns]\n",
    "columns.remove('day')\n",
    "# len(treatment_filt_norm)\n",
    "# all_values = []\n",
    "# for column in treatment_filt_norm:\n",
    "#     this_column_values = treatment_filt_norm[column].tolist()\n",
    "#     all_values += this_column_values\n",
    "# all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "# import numpy as np \n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# # antiviral = treatment_filt_norm['antiviral'].tolist()\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['antiviral'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#      name=\"antivirals\"    \n",
    "# )\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['plasma'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#     name=\"plasma\"\n",
    "# )\n",
    "# data = go.Data([trace0, trace1])\n",
    "\n",
    "# plotly.offline.iplot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm\n",
    "col = []\n",
    "for i in treatment_filt_norm:\n",
    "    x = 'freq_'+i.title()\n",
    "    col.append(x)\n",
    "    \n",
    "# col\n",
    "\n",
    "treatment_filt_norm.columns = col\n",
    "treatment_filt_norm\n",
    "\n",
    "\n",
    "# treatment_filt_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "treatment_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# treatment_filt_norm.info()\n",
    "treatment_filt_norm = treatment_filt_norm.rename({'freq_Day': 'Day'}, axis=1)\n",
    "treatment_filt_norm = treatment_filt_norm.rename({'freq_Vitamin-D': 'freq_Vitamind'}, axis=1)\n",
    "treatment_filt_norm = treatment_filt_norm.rename({'freq_Vitamin-C': 'freq_Vitaminc'}, axis=1)\n",
    "treatment_filt_norm = treatment_filt_norm.rename({'freq_Monoclonal-Antibody': 'freq_MonoclonalAntibody'}, axis=1)\n",
    "\n",
    "treatment_filt_norm.reset_index(inplace=True)\n",
    "\n",
    "long= pd.wide_to_long(treatment_filt_norm, stubnames='freq', sep='_',\n",
    "                      i=['Day'], j='term', suffix=r'\\w+')\n",
    "long.reset_index(inplace=True)\n",
    "long_treatment = long.filter(['Day','term','freq'], axis=1)\n",
    "long_treatment\n",
    "\n",
    "long_treatment['term'].unique()\n",
    "\n",
    "long_treatment = long_treatment.replace(to_replace =\"Vitamind\",\n",
    "                 value =\"Vitamin D\")\n",
    "long_treatment= long_treatment.replace(to_replace =\"Vitaminc\",\n",
    "                 value =\"Vitamin C\")\n",
    "long_treatment= long_treatment.replace(to_replace =\"Rna\",\n",
    "                 value =\"RNA\")\n",
    "long_treatment= long_treatment.replace(to_replace =\"MonoclonalAntibody\",\n",
    "                 value =\"Monoclonal antibody\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_treatment.term.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode() \n",
    "\n",
    "fig = px.area(long_treatment, x='Day', y = 'freq', color = 'term',\n",
    "               width=970, height=2000, line_shape='spline', facet_row=\"term\",\n",
    "              template=\"ggplot2\",\n",
    "             labels={ # replaces default labels by column name\n",
    "                'term':'Term', \"Day\": \"Session date\", \"freq\": \"Frequency score\"})\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text='',  nticks=2 )\n",
    "\n",
    "fig.update_xaxes(ticklabelposition= 'inside top')\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend = False, yaxis={'visible': False, 'showticklabels': False}, font_family=\"Gill Sans\")\n",
    "\n",
    "#     yaxis=dict(\n",
    "#         autorange=True,\n",
    "#         showgrid=False,\n",
    "#         ticks='',\n",
    "#         showticklabels=False\n",
    "\n",
    "\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "\n",
    "for annotation in fig['layout']['annotations']: \n",
    "    annotation['textangle']= 0\n",
    "    annotation['yshift']=40\n",
    "    annotation['font'] = dict(\n",
    "                color=\"black\",\n",
    "                size=11)\n",
    "    annotation['x']=0.45\n",
    "\n",
    "    \n",
    "# fig.update_xaxes(rangeslider= {'visible':True}, type=\"date\", row=1, col=1)\n",
    "    \n",
    "fig.show()\n",
    "fig.write_html(\"../graphs/therapies.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symptoms-related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "symptoms = (lexicon[\"type\"] == 'symptom')\n",
    "df_symptom = lexicon.loc[symptoms]\n",
    "df_symptom\n",
    "\n",
    "vocab_symptom = df_symptom['word'].tolist()\n",
    "len(vocab_symptom)\n",
    "vocab_symptom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "count_dict={}\n",
    "for v in vocab_symptom:\n",
    "    items=[]\n",
    "    x=1\n",
    "    while x<=len(trans_list_clean):\n",
    "        items.append(0)\n",
    "        x+=1\n",
    "    count_dict[v]=items\n",
    "#     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "count_dict\n",
    "\n",
    "df = pd.DataFrame(count_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trans_list_clean)):\n",
    "    words=trans_list_clean[i].split()\n",
    "    listemmed = stemmer.stem(trans_list_clean[i].lower())\n",
    "#     count=0\n",
    "#     filtered = []\n",
    "    for word in words:\n",
    "        word_stemmed = stemmer.stem(word.lower())\n",
    "        count=0\n",
    "        count = vocab_symptom.count(word.lower())\n",
    "        if count==1:\n",
    "            print(word.lower())\n",
    "            count_dict[word.lower()][i]+=1\n",
    "#         print(word_stemmed)\n",
    "#     if word_stemmed in vocabulary:\n",
    "#         filtered.append(word_stemmed)\n",
    "\n",
    "count_dict\n",
    "\n",
    "# Counter(words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "symp = pd.DataFrame(count_dict)\n",
    "symp\n",
    "symp.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with zeros only\n",
    "symp_filt=symp.loc[(symp.sum(axis=1) != 0), (symp.sum(axis=0) != 0)]\n",
    "symp_filt.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "symp_filt=symp_filt.drop(['symptom', 'symptoms', 'non-critical-covid', 'mild-covid'], axis = 1)\n",
    "\n",
    "symp_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "symp_filt_norm = min_max_scaling(symp_filt)\n",
    "\n",
    "symp_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_string = ''.join(trans_list_clean[0])\n",
    "joined_string[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "# tokens.remove('')        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame({'day': ['4/15/2020', '4/20/2020', '4/22/2020', \n",
    "                              '4/27/2020', '5/4/2020', '5/11/2020', '5/18/2020', '6/1/2020', '6/8/2020', \n",
    "                            '6/15/2020', '6/29/2020', '7/13/2020', '7/20/2020', '7/27/2020', \n",
    "                              '8/3/2020', '8/10/2020', '8/17/2020', '8/24/2020', '8/31/2020',\n",
    "                            '9/14/2020', '9/21/2020',  '9/28/2020', '10/5/2020', '10/20/2020',\n",
    "                             '10/26/2020', '11/2/2020', '11/9/2020',\n",
    "                             '11/16/2020', '11/30/2020', '12/7/2020', \n",
    "                              '12/14/2020', '12/21/2020', '1/4/2021',\n",
    "                             '1/11/2021', '1/25/2021', '2/1/2021', \n",
    "                              '2/8/2021', '2/15/2021', '2/22/2021',\n",
    "                             '3/8/2021', '3/15/2021', '3/22/2021', \n",
    "                              '3/29/2021', '4/5/2021', '4/12/2021',\n",
    "                             '4/19/2021', '4/26/2021', '5/10/2021']})\n",
    "\n",
    "symp_filt_norm['day'] = dates\n",
    "symp_filt_norm['day'] = pd.to_datetime(symp_filt_norm['day'])\n",
    "symp_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm.to_csv('../data/clean/test.csv')\n",
    "symp_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_low = symp_filt_norm.columns.values.tolist()\n",
    "columns_title=[]\n",
    "for c in columns_low:\n",
    "    x=c.title()\n",
    "    columns_title.append(x)\n",
    "symp_filt_norm.reset_index(drop=True, inplace=True)\n",
    "columns_title\n",
    "treatment_filt_norm\n",
    "columns=['asymptomatic', \n",
    " 'symptomatic',\n",
    " 'inflammatory-markers',\n",
    " 'dyspnea',\n",
    "  'pneumonia',\n",
    "  'appetite',\n",
    "  'diagnosis',\n",
    " 'severe-covid',\n",
    " 'fatigue',\n",
    "  'chills',\n",
    "  'anosmia',\n",
    " 'fever',\n",
    " 'cough',\n",
    " 'sore-throat',\n",
    " 'diarrhea',\n",
    " 'rashes',\n",
    " 'clotting',\n",
    " 'day']\n",
    "\n",
    "# symp_filt_norm.info()\n",
    "\n",
    "symp_filt_norm = symp_filt_norm[columns]\n",
    "\n",
    "\n",
    "columns.remove('day')\n",
    "# len(treatment_filt_norm)\n",
    "# all_values = []\n",
    "# for column in treatment_filt_norm:\n",
    "#     this_column_values = treatment_filt_norm[column].tolist()\n",
    "#     all_values += this_column_values\n",
    "# all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "# import numpy as np \n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# # antiviral = treatment_filt_norm['antiviral'].tolist()\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['antiviral'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#      name=\"antivirals\"    \n",
    "# )\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['plasma'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#     name=\"plasma\"\n",
    "# )\n",
    "# data = go.Data([trace0, trace1])\n",
    "\n",
    "# plotly.offline.iplot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm\n",
    "col = []\n",
    "for i in symp_filt_norm:\n",
    "    x = 'freq_'+i.title()\n",
    "    col.append(x)\n",
    "    \n",
    "# col\n",
    "\n",
    "symp_filt_norm.columns = col\n",
    "symp_filt_norm\n",
    "\n",
    "\n",
    "# treatment_filt_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "symp_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# treatment_filt_norm.info()\n",
    "symp_filt_norm = symp_filt_norm.rename({'freq_Day': 'Day'}, axis=1)\n",
    "symp_filt_norm = symp_filt_norm.rename({'freq_Severe-Covid': 'freq_SevereCovid'}, axis=1)\n",
    "symp_filt_norm = symp_filt_norm.rename({'freq_Sore-Throat': 'freq_SoreThroat'}, axis=1)\n",
    "symp_filt_norm = symp_filt_norm.rename({'freq_Inflammatory-Markers': 'freq_InflammatoryMarkers'}, axis=1)\n",
    "\n",
    "symp_filt_norm.reset_index(inplace=True)\n",
    "\n",
    "long= pd.wide_to_long(symp_filt_norm, stubnames='freq', sep='_',\n",
    "                      i=['Day'], j='term', suffix=r'\\w+')\n",
    "long.reset_index(inplace=True)\n",
    "long_symp = long.filter(['Day','term','freq'], axis=1)\n",
    "long_symp\n",
    "\n",
    "long_symp['term'].unique()\n",
    "\n",
    "long_symp = long_symp.replace(to_replace =\"SevereCovid\",\n",
    "                 value =\"Severe COVID\")\n",
    "long_symp= long_symp.replace(to_replace =\"SoreThroat\",\n",
    "                 value =\"Sore throat\")\n",
    "long_symp= long_symp.replace(to_replace =\"InflammatoryMarkers\",\n",
    "                 value =\"Inflammatory markers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_symp.term.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode() \n",
    "\n",
    "fig = px.area(long_symp, x='Day', y = 'freq', color = 'term',\n",
    "               width=970, height=2000, line_shape='spline', facet_row=\"term\",\n",
    "              template=\"ggplot2\",\n",
    "             labels={ # replaces default labels by column name\n",
    "                'term':'Term', \"Day\": \"Session date\", \"freq\": \"Frequency score\"})\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text='',  nticks=2 )\n",
    "\n",
    "fig.update_xaxes(ticklabelposition= 'inside top')\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend = False, yaxis={'visible': False, 'showticklabels': False}, font_family=\"Gill Sans\")\n",
    "\n",
    "#     yaxis=dict(\n",
    "#         autorange=True,\n",
    "#         showgrid=False,\n",
    "#         ticks='',\n",
    "#         showticklabels=False\n",
    "\n",
    "\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "\n",
    "for annotation in fig['layout']['annotations']: \n",
    "    annotation['textangle']= 0\n",
    "    annotation['yshift']=40\n",
    "    annotation['font'] = dict(\n",
    "                color=\"black\",\n",
    "                size=11)\n",
    "    annotation['x']=0.45\n",
    "\n",
    "    \n",
    "# fig.update_xaxes(rangeslider= {'visible':True}, type=\"date\", row=1, col=1)\n",
    "    \n",
    "fig.show()\n",
    "fig.write_html(\"../graphs/symptoms.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spread-related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spread = (lexicon[\"type\"] == 'spread')\n",
    "df_spread = lexicon.loc[spread]\n",
    "df_spread\n",
    "\n",
    "vocab_spread = df_spread['word'].tolist()\n",
    "len(vocab_spread)\n",
    "vocab_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "count_dict={}\n",
    "for v in vocab_spread:\n",
    "    items=[]\n",
    "    x=1\n",
    "    while x<=len(trans_list_clean):\n",
    "        items.append(0)\n",
    "        x+=1\n",
    "    count_dict[v]=items\n",
    "#     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "count_dict\n",
    "\n",
    "df = pd.DataFrame(count_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trans_list_clean)):\n",
    "    words=trans_list_clean[i].split()\n",
    "    listemmed = stemmer.stem(trans_list_clean[i].lower())\n",
    "#     count=0\n",
    "#     filtered = []\n",
    "    for word in words:\n",
    "        word_stemmed = stemmer.stem(word.lower())\n",
    "        count=0\n",
    "        count = vocab_spread.count(word.lower())\n",
    "        if count==1:\n",
    "            print(word.lower())\n",
    "            count_dict[word.lower()][i]+=1\n",
    "#         print(word_stemmed)\n",
    "#     if word_stemmed in vocabulary:\n",
    "#         filtered.append(word_stemmed)\n",
    "\n",
    "count_dict\n",
    "\n",
    "# Counter(words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sprea = pd.DataFrame(count_dict)\n",
    "sprea\n",
    "sprea.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with zeros only\n",
    "sprea_filt=sprea.loc[(sprea.sum(axis=1) != 0), (sprea.sum(axis=0) != 0)]\n",
    "sprea_filt.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "sprea_filt=sprea_filt.drop(['antibody', 'tracing', 'diagnostic', 'flatten', 'infection', 'diagnosis'], axis = 1)\n",
    "sprea_filt.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "sprea_filt.columns\n",
    "sprea_filt=sprea_filt.drop([ 'quarantine',\n",
    " 'isolation',\n",
    " 'lock-down',\n",
    " 'mask',\n",
    " 'n95',\n",
    "#  'n 95',\n",
    " 'ppe',\n",
    "#  'personal-protective-equipment',\n",
    "#  'protective-equipment',\n",
    "#  'personal-equipment',\n",
    " 'npis',\n",
    " 'hand-washing',\n",
    " 'hygiene',\n",
    "#  'hygienic',\n",
    "#  'social-distancing',\n",
    " 'social-distance',\n",
    " 'sanitizer'], axis = 1)\n",
    "\n",
    "# sprea_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "sprea_filt_norm = min_max_scaling(sprea_filt)\n",
    "\n",
    "sprea_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_string = ''.join(trans_list_clean[0])\n",
    "joined_string[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "# tokens.remove('')        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame({'day': ['4/15/2020', '4/20/2020', '4/22/2020', \n",
    "                              '4/27/2020', '5/4/2020', '5/11/2020', '5/18/2020', '6/1/2020', '6/8/2020', \n",
    "                            '6/15/2020', '6/29/2020', '7/13/2020', '7/20/2020', '7/27/2020', \n",
    "                              '8/3/2020', '8/10/2020', '8/17/2020', '8/24/2020', '8/31/2020',\n",
    "                            '9/14/2020', '9/21/2020',  '9/28/2020', '10/5/2020', '10/20/2020',\n",
    "                             '10/26/2020', '11/2/2020', '11/9/2020',\n",
    "                             '11/16/2020', '11/30/2020', '12/7/2020', \n",
    "                              '12/14/2020', '12/21/2020', '1/4/2021',\n",
    "                             '1/11/2021', '1/25/2021', '2/1/2021', \n",
    "                              '2/8/2021', '2/15/2021', '2/22/2021',\n",
    "                             '3/8/2021', '3/15/2021', '3/22/2021', \n",
    "                              '3/29/2021', '4/5/2021', '4/12/2021',\n",
    "                             '4/19/2021', '4/26/2021', '5/10/2021']})\n",
    "\n",
    "sprea_filt_norm['day'] = dates\n",
    "sprea_filt_norm['day'] = pd.to_datetime(sprea_filt_norm['day'])\n",
    "sprea_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm.to_csv('../data/clean/test.csv')\n",
    "sprea_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_low = sprea_filt_norm.columns.values.tolist()\n",
    "columns_title=[]\n",
    "for c in columns_low:\n",
    "    x=c.title()\n",
    "    columns_title.append(x)\n",
    "symp_filt_norm.reset_index(drop=True, inplace=True)\n",
    "columns_low\n",
    "# sprea_filt_norm\n",
    "\n",
    "# columns = ['screening',\n",
    "#             'contact-tracing',\n",
    "#  'blood-test',\n",
    "#  'false-negative',\n",
    "#  'false-positive',\n",
    "#  'antibody-test',\n",
    "#  'test',\n",
    "#             'pcr',\n",
    "#  'lateral-flow-test',\n",
    "#  'pooled-testing',\n",
    "#             'rapid-test',\n",
    "#  'saliva-test',\n",
    "#  'saliva',\n",
    "# 'swab-test',\n",
    "#  'point-of-care',           \n",
    "#  's-gene',           \n",
    "#            'accuracy',\n",
    "#  'sensitivity',\n",
    "#  'specificity',    \n",
    "#  'sequencing',           \n",
    "#  'antigen',\n",
    "#  'confirmed',\n",
    "#  'suspected',\n",
    "           \n",
    "#  'quarantine',\n",
    "#  'isolation',\n",
    "#  'mask',\n",
    "#  'n95',\n",
    "#  'ppe',\n",
    "#  'fomite',\n",
    "#  'aerosol',\n",
    "#  'airborne',\n",
    "#  'droplet',\n",
    "\n",
    "\n",
    "#  'asymptomatic',\n",
    "\n",
    "# 'case-fatality',\n",
    "#  'fatality',\n",
    "#  'index-case',\n",
    "#  'epidemiology',\n",
    "# #  'epidemiologist',\n",
    "# 'outbreak',           \n",
    "#  'flatten-the-curve',\n",
    "#  'wave',           \n",
    "#  'curve',\n",
    "#  'spread',\n",
    "#  'contagion',\n",
    "#  'transmissibility',\n",
    "#  'transmission',           \n",
    "#  'containment',           \n",
    "#  'infection-fatality',           \n",
    "#  'immunity',\n",
    "#  'incidence',\n",
    "#  'mortality',\n",
    "#  'prevalence',\n",
    "#  'reproduction-number',           \n",
    "# 'incubation',\n",
    "#  'superspreader',\n",
    "#  'exposed',\n",
    "#  'infectious',\n",
    "#  'hand-washing',\n",
    "#  'hygiene',\n",
    "# #  'hygienic',\n",
    "#  'social-distance',\n",
    "#  'serologic',\n",
    "#  'sanitizer',\n",
    "#  'npis',\n",
    "#  'day']\n",
    "\n",
    "sprea_filt_norm = sprea_filt_norm[columns_low]\n",
    "columns=columns_low \n",
    "\n",
    "# columns.remove('day')\n",
    "# columns_low.remove('day')\n",
    "# len(treatment_filt_norm)\n",
    "# all_values = []\n",
    "# for column in treatment_filt_norm:\n",
    "#     this_column_values = treatment_filt_norm[column].tolist()\n",
    "#     all_values += this_column_values\n",
    "# all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "# import numpy as np \n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# # antiviral = treatment_filt_norm['antiviral'].tolist()\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['antiviral'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#      name=\"antivirals\"    \n",
    "# )\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['plasma'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#     name=\"plasma\"\n",
    "# )\n",
    "# data = go.Data([trace0, trace1])\n",
    "\n",
    "# plotly.offline.iplot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm\n",
    "col = []\n",
    "for i in sprea_filt_norm:\n",
    "    x = 'freq_'+i.title()\n",
    "    col.append(x)\n",
    "    \n",
    "# col\n",
    "\n",
    "sprea_filt_norm.columns = col\n",
    "sprea_filt_norm\n",
    "\n",
    "\n",
    "# treatment_filt_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sprea_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# treatment_filt_norm.info()\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Day': 'Day'}, axis=1)\n",
    "# sprea_filt_norm = sprea_filt_norm.rename({'freq_Social-Distance': 'freq_SocialDistance'}, axis=1)\n",
    "# sprea_filt_norm = sprea_filt_norm.rename({'freq_Hand-Washing': 'freq_HandWashing'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Saliva-Test': 'freq_SalivaTest'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Rapid-Test': 'freq_RapidTest'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Reproduction-Number': 'freq_ReproductionNumber'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Pooled-Testing': 'freq_PooledTesting'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Point-Of-Care': 'freq_PointOfCare'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Mass-Testing': 'freq_MassTesting'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Lateral-Flow-Test': 'freq_LateralFlowTest'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Index-Case': 'freq_IndexCase'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Infection-Fatality': 'freq_InfectionFatality'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Flatten-The-Curve': 'freq_FlattenTheCurve'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_False-Positive': 'freq_FalsePositive'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_False-Negative': 'freq_FalseNegative'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Contact-Tracing': 'freq_ContactTracing'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Case-Fatality': 'freq_CaseFatality'}, axis=1)\n",
    "sprea_filt_norm = sprea_filt_norm.rename({'freq_Antibody-Test': 'freq_AntibodyTest'}, axis=1)\n",
    "sprea_filt_norm.reset_index(inplace=True)\n",
    "\n",
    "long= pd.wide_to_long(sprea_filt_norm, stubnames='freq', sep='_',\n",
    "                      i=['Day'], j='term', suffix=r'\\w+')\n",
    "long.reset_index(inplace=True)\n",
    "long_sprea = long.filter(['Day','term','freq'], axis=1)\n",
    "long_sprea\n",
    "\n",
    "long_sprea['term'].unique()\n",
    "\n",
    "# long_sprea = long_sprea.replace(to_replace =\"SocialDistance\",\n",
    "#                  value =\"Social distance\")\n",
    "# long_sprea= long_sprea.replace(to_replace =\"HandWashing\",\n",
    "#                  value =\"Hand washing\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"SalivaTest\",\n",
    "                 value =\"Saliva test\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"RapidTest\",\n",
    "                 value =\"Rapid test\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"ReproductionNumber\",\n",
    "                 value =\"Reproduction number\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"PooledTesting\",\n",
    "                 value =\"Pooled testing\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"PointOfCare\",\n",
    "                 value =\"Point-of-care test\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"MassTesting\",\n",
    "                 value =\"Mass testing\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"LateralFlowTest\",\n",
    "                 value =\"Lateral flow test\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"IndexCase\",\n",
    "                 value =\"Index case\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"InfectionFatality\",\n",
    "                 value =\"Infection-fatality\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"FlattenTheCurve\",\n",
    "                 value =\"Flatten the curve\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"FalsePositive\",\n",
    "                 value =\"False positive\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"FalseNegative\",\n",
    "                 value =\"False negative\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"ContactTracing\",\n",
    "                 value =\"Contact tracing\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"CaseFatality\",\n",
    "                 value =\"Case fatality\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"AntibodyTest\",\n",
    "                 value =\"Antibody test\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"Pcr\",\n",
    "                 value =\"PCR\")\n",
    "long_sprea= long_sprea.replace(to_replace =\"Npis\",\n",
    "                 value =\"NPIs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sprea.term.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()  \n",
    "fig = px.area(long_sprea, x='Day', y = 'freq', color = 'term',\n",
    "               width=970, height=5000, line_shape='spline', facet_row=\"term\", facet_row_spacing = 0.01,\n",
    "              template=\"ggplot2\",\n",
    "             labels={ # replaces default labels by column name\n",
    "                'term':'Term', \"Day\": \"Session date\", \"freq\": \"Frequency score\"})\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text='',  nticks=2 )\n",
    "\n",
    "fig.update_xaxes(ticklabelposition= 'inside top')\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend = False, yaxis={'visible': False, 'showticklabels': False}, font_family=\"Gill Sans\")\n",
    "\n",
    "#     yaxis=dict(\n",
    "#         autorange=True,\n",
    "#         showgrid=False,\n",
    "#         ticks='',\n",
    "#         showticklabels=False\n",
    "\n",
    "\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "\n",
    "for annotation in fig['layout']['annotations']: \n",
    "    annotation['textangle']= 0\n",
    "    annotation['yshift']=40\n",
    "    annotation['font'] = dict(\n",
    "                color=\"black\",\n",
    "                size=11)\n",
    "    annotation['x']=0.45\n",
    "\n",
    "    \n",
    "# fig.update_xaxes(rangeslider= {'visible':True}, type=\"date\", row=1, col=1)\n",
    "    \n",
    "fig.show()\n",
    "fig.write_html(\"../graphs/spread.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPI-related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npi = (lexicon[\"type\"] == 'spread')\n",
    "df_npi = lexicon.loc[spread]\n",
    "df_npi\n",
    "\n",
    "vocab_npi = df_npi['word'].tolist()\n",
    "len(vocab_npi)\n",
    "vocab_npi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "count_dict={}\n",
    "for v in vocab_npi:\n",
    "    items=[]\n",
    "    x=1\n",
    "    while x<=len(trans_list_clean):\n",
    "        items.append(0)\n",
    "        x+=1\n",
    "    count_dict[v]=items\n",
    "#     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "count_dict\n",
    "\n",
    "df = pd.DataFrame(count_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trans_list_clean)):\n",
    "    words=trans_list_clean[i].split()\n",
    "    listemmed = stemmer.stem(trans_list_clean[i].lower())\n",
    "#     count=0\n",
    "#     filtered = []\n",
    "    for word in words:\n",
    "        word_stemmed = stemmer.stem(word.lower())\n",
    "        count=0\n",
    "        count = vocab_npi.count(word.lower())\n",
    "        if count==1:\n",
    "            print(word.lower())\n",
    "            count_dict[word.lower()][i]+=1\n",
    "#         print(word_stemmed)\n",
    "#     if word_stemmed in vocabulary:\n",
    "#         filtered.append(word_stemmed)\n",
    "\n",
    "count_dict\n",
    "\n",
    "# Counter(words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npi = pd.DataFrame(count_dict)\n",
    "npi\n",
    "npi.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with zeros only\n",
    "npi_filt=sprea.loc[(npi.sum(axis=1) != 0), (npi.sum(axis=0) != 0)]\n",
    "npi_filt.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "npi_filt=npi_filt.drop(['antibody', 'tracing', 'diagnostic', 'flatten', 'infection', 'diagnosis'], axis = 1)\n",
    "npi_filt.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "npi_filt.columns\n",
    "npi_filt=npi_filt.drop([ 'screening',\n",
    " 'outbreak',\n",
    " 'accuracy',\n",
    " 'antibody-test',\n",
    " 'asymptomatic',\n",
    " 'case-fatality',\n",
    " 'fatality',\n",
    " 'contact-tracing',\n",
    " 'blood-test',\n",
    " 'epidemiology',\n",
    " 'false-negative',\n",
    " 'false-positive',\n",
    " 'flatten-the-curve',\n",
    " 'curve',\n",
    " 'immunity',\n",
    " 'incidence',\n",
    " 'incubation',\n",
    " 'infection-fatality',\n",
    " 'index-case',\n",
    " 'test',\n",
    " 'lateral-flow-test',\n",
    " 'mass-testing',\n",
    " 'mortality',\n",
    " 'pcr',\n",
    " 'point-of-care',\n",
    " 'pooled-testing',\n",
    " 'prevalence',\n",
    " 'reproduction-number',\n",
    " 'rapid-test',\n",
    " 'saliva-test',\n",
    " 'saliva',\n",
    " 's-gene',\n",
    " 'sensitivity',\n",
    " 'sequencing',\n",
    " 'specificity',\n",
    " 'superspreader',\n",
    " 'swab-test',\n",
    " 'transmissibility',\n",
    " 'transmission',\n",
    " 'confirmed',\n",
    " 'suspected',\n",
    " 'exposed',\n",
    " 'infectious',\n",
    " 'serologic',\n",
    " 'sanitizer',\n",
    " 'spread',\n",
    " 'contagion',\n",
    " 'wave',\n",
    " 'containment'\n",
    "], axis = 1)\n",
    "\n",
    "# npi_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "npi_filt_norm = min_max_scaling(npi_filt)\n",
    "\n",
    "npi_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_string = ''.join(trans_list_clean[0])\n",
    "joined_string[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "# tokens.remove('')        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame({'day': ['4/15/2020', '4/20/2020', '4/22/2020', \n",
    "                              '4/27/2020', '5/4/2020', '5/11/2020', '5/18/2020', '6/1/2020', '6/8/2020', \n",
    "                            '6/15/2020', '6/29/2020', '7/13/2020', '7/20/2020', '7/27/2020', \n",
    "                              '8/3/2020', '8/10/2020', '8/17/2020', '8/24/2020', '8/31/2020',\n",
    "                            '9/14/2020', '9/21/2020',  '9/28/2020', '10/5/2020', '10/20/2020',\n",
    "                             '10/26/2020', '11/2/2020', '11/9/2020',\n",
    "                             '11/16/2020', '11/30/2020', '12/7/2020', \n",
    "                              '12/14/2020', '12/21/2020', '1/4/2021',\n",
    "                             '1/11/2021', '1/25/2021', '2/1/2021', \n",
    "                              '2/8/2021', '2/15/2021', '2/22/2021',\n",
    "                             '3/8/2021', '3/15/2021', '3/22/2021', \n",
    "                              '3/29/2021', '4/5/2021', '4/12/2021',\n",
    "                             '4/19/2021', '4/26/2021', '5/10/2021']})\n",
    "\n",
    "npi_filt_norm['day'] = dates\n",
    "npi_filt_norm['day'] = pd.to_datetime(npi_filt_norm['day'])\n",
    "npi_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm.to_csv('../data/clean/test.csv')\n",
    "npi_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_low = npi_filt_norm.columns.values.tolist()\n",
    "columns_title=[]\n",
    "for c in columns_low:\n",
    "    x=c.title()\n",
    "    columns_title.append(x)\n",
    "npi_filt_norm.reset_index(drop=True, inplace=True)\n",
    "columns_low\n",
    "# sprea_filt_norm\n",
    "\n",
    "# columns = ['screening',\n",
    "#             'contact-tracing',\n",
    "#  'blood-test',\n",
    "#  'false-negative',\n",
    "#  'false-positive',\n",
    "#  'antibody-test',\n",
    "#  'test',\n",
    "#             'pcr',\n",
    "#  'lateral-flow-test',\n",
    "#  'pooled-testing',\n",
    "#             'rapid-test',\n",
    "#  'saliva-test',\n",
    "#  'saliva',\n",
    "# 'swab-test',\n",
    "#  'point-of-care',           \n",
    "#  's-gene',           \n",
    "#            'accuracy',\n",
    "#  'sensitivity',\n",
    "#  'specificity',    \n",
    "#  'sequencing',           \n",
    "#  'antigen',\n",
    "#  'confirmed',\n",
    "#  'suspected',\n",
    "           \n",
    "#  'quarantine',\n",
    "#  'isolation',\n",
    "#  'mask',\n",
    "#  'n95',\n",
    "#  'ppe',\n",
    "#  'fomite',\n",
    "#  'aerosol',\n",
    "#  'airborne',\n",
    "#  'droplet',\n",
    "\n",
    "\n",
    "#  'asymptomatic',\n",
    "\n",
    "# 'case-fatality',\n",
    "#  'fatality',\n",
    "#  'index-case',\n",
    "#  'epidemiology',\n",
    "# #  'epidemiologist',\n",
    "# 'outbreak',           \n",
    "#  'flatten-the-curve',\n",
    "#  'wave',           \n",
    "#  'curve',\n",
    "#  'spread',\n",
    "#  'contagion',\n",
    "#  'transmissibility',\n",
    "#  'transmission',           \n",
    "#  'containment',           \n",
    "#  'infection-fatality',           \n",
    "#  'immunity',\n",
    "#  'incidence',\n",
    "#  'mortality',\n",
    "#  'prevalence',\n",
    "#  'reproduction-number',           \n",
    "# 'incubation',\n",
    "#  'superspreader',\n",
    "#  'exposed',\n",
    "#  'infectious',\n",
    "#  'hand-washing',\n",
    "#  'hygiene',\n",
    "# #  'hygienic',\n",
    "#  'social-distance',\n",
    "#  'serologic',\n",
    "#  'sanitizer',\n",
    "#  'npis',\n",
    "#  'day']\n",
    "\n",
    "npi_filt_norm = npi_filt_norm[columns_low]\n",
    "columns=columns_low \n",
    "\n",
    "# columns.remove('day')\n",
    "# columns_low.remove('day')\n",
    "# len(treatment_filt_norm)\n",
    "# all_values = []\n",
    "# for column in treatment_filt_norm:\n",
    "#     this_column_values = treatment_filt_norm[column].tolist()\n",
    "#     all_values += this_column_values\n",
    "# all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "# import numpy as np \n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# # antiviral = treatment_filt_norm['antiviral'].tolist()\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['antiviral'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#      name=\"antivirals\"    \n",
    "# )\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['plasma'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#     name=\"plasma\"\n",
    "# )\n",
    "# data = go.Data([trace0, trace1])\n",
    "\n",
    "# plotly.offline.iplot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm\n",
    "col = []\n",
    "for i in npi_filt_norm:\n",
    "    x = 'freq_'+i.title()\n",
    "    col.append(x)\n",
    "    \n",
    "# col\n",
    "\n",
    "npi_filt_norm.columns = col\n",
    "npi_filt_norm\n",
    "\n",
    "\n",
    "# treatment_filt_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npi_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# treatment_filt_norm.info()\n",
    "npi_filt_norm = npi_filt_norm.rename({'freq_Day': 'Day'}, axis=1)\n",
    "npi_filt_norm = npi_filt_norm.rename({'freq_Social-Distance': 'freq_SocialDistance'}, axis=1)\n",
    "npi_filt_norm = npi_filt_norm.rename({'freq_Lock-Down': 'freq_Lockdown'}, axis=1)\n",
    "npi_filt_norm.reset_index(inplace=True)\n",
    "\n",
    "npi_filt_norm=npi_filt_norm.drop(['freq_Npis'], axis = 1)\n",
    "\n",
    "long= pd.wide_to_long(npi_filt_norm, stubnames='freq', sep='_',\n",
    "                      i=['Day'], j='term', suffix=r'\\w+')\n",
    "long.reset_index(inplace=True)\n",
    "long_npi = long.filter(['Day','term','freq'], axis=1)\n",
    "long_npi\n",
    "\n",
    "long_npi['term'].unique()\n",
    "\n",
    "long_npi = long_npi.replace(to_replace =\"SocialDistance\",\n",
    "                 value =\"Social distance\")\n",
    "long_npi= long_npi.replace(to_replace =\"HandWashing\",\n",
    "                 value =\"Hand washing\")\n",
    "long_npi= long_npi.replace(to_replace =\"Ppe\",\n",
    "                 value =\"Personal protective equipment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_npi.term.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()  \n",
    "fig = px.area(long_npi, x='Day', y = 'freq', color = 'term',\n",
    "               width=970, height=1500, line_shape='spline', facet_row=\"term\",\n",
    "              template=\"ggplot2\",\n",
    "             labels={ # replaces default labels by column name\n",
    "                'term':'Term', \"Day\": \"Session date\", \"freq\": \"Frequency score\"})\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text='',  nticks=2 )\n",
    "\n",
    "fig.update_xaxes(ticklabelposition= 'inside top')\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend = False, yaxis={'visible': False, 'showticklabels': False}, font_family=\"Gill Sans\")\n",
    "\n",
    "#     yaxis=dict(\n",
    "#         autorange=True,\n",
    "#         showgrid=False,\n",
    "#         ticks='',\n",
    "#         showticklabels=False\n",
    "\n",
    "\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "\n",
    "for annotation in fig['layout']['annotations']: \n",
    "    annotation['textangle']= 0\n",
    "    annotation['yshift']=45\n",
    "    annotation['font'] = dict(\n",
    "                color=\"black\",\n",
    "                size=11)\n",
    "    annotation['x']=0.45\n",
    "\n",
    "    \n",
    "# fig.update_xaxes(rangeslider= {'visible':True}, type=\"date\", row=1, col=1)\n",
    "    \n",
    "fig.show()\n",
    "fig.write_html(\"../graphs/npi.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biology-related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.type.unique()\n",
    "biology = (lexicon[\"type\"] == 'biology')\n",
    "df_biology = lexicon.loc[biology]\n",
    "df_biology\n",
    "\n",
    "vocab_biology = df_biology['word'].tolist()\n",
    "len(vocab_biology)\n",
    "vocab_biology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "count_dict={}\n",
    "for v in vocab_biology:\n",
    "    items=[]\n",
    "    x=1\n",
    "    while x<=len(trans_list_clean):\n",
    "        items.append(0)\n",
    "        x+=1\n",
    "    count_dict[v]=items\n",
    "#     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "count_dict\n",
    "\n",
    "df = pd.DataFrame(count_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trans_list_clean)):\n",
    "    words=trans_list_clean[i].split()\n",
    "    listemmed = stemmer.stem(trans_list_clean[i].lower())\n",
    "#     count=0\n",
    "#     filtered = []\n",
    "    for word in words:\n",
    "        word_stemmed = stemmer.stem(word.lower())\n",
    "        count=0\n",
    "        count = vocab_biology.count(word.lower())\n",
    "        if count==1:\n",
    "            print(word.lower())\n",
    "            count_dict[word.lower()][i]+=1\n",
    "#         print(word_stemmed)\n",
    "#     if word_stemmed in vocabulary:\n",
    "#         filtered.append(word_stemmed)\n",
    "\n",
    "count_dict\n",
    "\n",
    "# Counter(words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "biology = pd.DataFrame(count_dict)\n",
    "biology\n",
    "biology.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with zeros only\n",
    "biology_filt=biology.loc[(sprea.sum(axis=1) != 0), (biology.sum(axis=0) != 0)]\n",
    "biology_filt.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "biology_filt=biology_filt.drop(['reservoir', 'virology', 'coronavirus', 'covid-19'], axis = 1)\n",
    "biology_filt.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "# sprea_filt=sprea_filt.drop(['symptom', 'symptoms', 'non-critical-covid', 'mild-covid'], axis = 1)\n",
    "\n",
    "# sprea_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "biology_filt_norm = min_max_scaling(biology_filt)\n",
    "\n",
    "biology_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_string = ''.join(trans_list_clean[0])\n",
    "joined_string[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "# tokens.remove('')        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame({'day': ['4/15/2020', '4/20/2020', '4/22/2020', \n",
    "                              '4/27/2020', '5/4/2020', '5/11/2020', '5/18/2020', '6/1/2020', '6/8/2020', \n",
    "                            '6/15/2020', '6/29/2020', '7/13/2020', '7/20/2020', '7/27/2020', \n",
    "                              '8/3/2020', '8/10/2020', '8/17/2020', '8/24/2020', '8/31/2020',\n",
    "                            '9/14/2020', '9/21/2020',  '9/28/2020', '10/5/2020', '10/20/2020',\n",
    "                             '10/26/2020', '11/2/2020', '11/9/2020',\n",
    "                             '11/16/2020', '11/30/2020', '12/7/2020', \n",
    "                              '12/14/2020', '12/21/2020', '1/4/2021',\n",
    "                             '1/11/2021', '1/25/2021', '2/1/2021', \n",
    "                              '2/8/2021', '2/15/2021', '2/22/2021',\n",
    "                             '3/8/2021', '3/15/2021', '3/22/2021', \n",
    "                              '3/29/2021', '4/5/2021', '4/12/2021',\n",
    "                             '4/19/2021', '4/26/2021', '5/10/2021']})\n",
    "\n",
    "biology_filt_norm['day'] = dates\n",
    "biology_filt_norm['day'] = pd.to_datetime(biology_filt_norm['day'])\n",
    "biology_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm.to_csv('../data/clean/test.csv')\n",
    "biology_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_low = biology_filt_norm.columns.values.tolist()\n",
    "columns_title=[]\n",
    "for c in columns_low:\n",
    "    x=c.title()\n",
    "    columns_title.append(x)\n",
    "biology_filt_norm.reset_index(drop=True, inplace=True)\n",
    "columns_low\n",
    "# sprea_filt_norm\n",
    "\n",
    "# columns = ['screening',\n",
    "#             'contact-tracing',\n",
    "#  'blood-test',\n",
    "#  'false-negative',\n",
    "#  'false-positive',\n",
    "#  'antibody-test',\n",
    "#  'test',\n",
    "#             'pcr',\n",
    "#  'lateral-flow-test',\n",
    "#  'pooled-testing',\n",
    "#             'rapid-test',\n",
    "#  'saliva-test',\n",
    "#  'saliva',\n",
    "# 'swab-test',\n",
    "#  'point-of-care',           \n",
    "#  's-gene',           \n",
    "#            'accuracy',\n",
    "#  'sensitivity',\n",
    "#  'specificity',    \n",
    "#  'sequencing',           \n",
    "#  'antigen',\n",
    "#  'confirmed',\n",
    "#  'suspected',\n",
    "           \n",
    "#  'quarantine',\n",
    "#  'isolation',\n",
    "#  'mask',\n",
    "#  'n95',\n",
    "#  'ppe',\n",
    "#  'fomite',\n",
    "#  'aerosol',\n",
    "#  'airborne',\n",
    "#  'droplet',\n",
    "\n",
    "\n",
    "#  'asymptomatic',\n",
    "\n",
    "# 'case-fatality',\n",
    "#  'fatality',\n",
    "#  'index-case',\n",
    "#  'epidemiology',\n",
    "# #  'epidemiologist',\n",
    "# 'outbreak',           \n",
    "#  'flatten-the-curve',\n",
    "#  'wave',           \n",
    "#  'curve',\n",
    "#  'spread',\n",
    "#  'contagion',\n",
    "#  'transmissibility',\n",
    "#  'transmission',           \n",
    "#  'containment',           \n",
    "#  'infection-fatality',           \n",
    "#  'immunity',\n",
    "#  'incidence',\n",
    "#  'mortality',\n",
    "#  'prevalence',\n",
    "#  'reproduction-number',           \n",
    "# 'incubation',\n",
    "#  'superspreader',\n",
    "#  'exposed',\n",
    "#  'infectious',\n",
    "#  'hand-washing',\n",
    "#  'hygiene',\n",
    "#  'hygienic',\n",
    "#  'social-distance',\n",
    "#  'serologic',\n",
    "#  'sanitizer',\n",
    "#  'npis',\n",
    "#  'day']\n",
    "\n",
    "biology_filt_norm = biology_filt_norm[columns_low]\n",
    "columns=columns_low \n",
    "\n",
    "columns.remove('day')\n",
    "# len(treatment_filt_norm)\n",
    "# all_values = []\n",
    "# for column in treatment_filt_norm:\n",
    "#     this_column_values = treatment_filt_norm[column].tolist()\n",
    "#     all_values += this_column_values\n",
    "# all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "# import numpy as np \n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# # antiviral = treatment_filt_norm['antiviral'].tolist()\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['antiviral'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#      name=\"antivirals\"    \n",
    "# )\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['plasma'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#     name=\"plasma\"\n",
    "# )\n",
    "# data = go.Data([trace0, trace1])\n",
    "\n",
    "# plotly.offline.iplot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm\n",
    "col = []\n",
    "for i in biology_filt_norm:\n",
    "    x = 'freq_'+i.title()\n",
    "    col.append(x)\n",
    "    \n",
    "# col\n",
    "\n",
    "biology_filt_norm.columns = col\n",
    "biology_filt_norm\n",
    "\n",
    "\n",
    "# treatment_filt_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "biology_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# treatment_filt_norm.info()\n",
    "biology_filt_norm = biology_filt_norm.rename({'freq_Day': 'Day'}, axis=1)\n",
    "biology_filt_norm = biology_filt_norm.rename({'freq_Viral-Shedding': 'freq_ViralShedding'}, axis=1)\n",
    "biology_filt_norm = biology_filt_norm.rename({'freq_Viral-Load': 'freq_ViralLoad'}, axis=1)\n",
    "# biology_filt_norm.reset_index(inplace=True)\n",
    "\n",
    "long= pd.wide_to_long(biology_filt_norm, stubnames='freq', sep='_',\n",
    "                      i=['Day'], j='term', suffix=r'\\w+')\n",
    "long.reset_index(inplace=True)\n",
    "long_biology = long.filter(['Day','term','freq'], axis=1)\n",
    "# long_biology.columns\n",
    "\n",
    "long_biology= long_biology.replace(to_replace =\"ViralShedding\",\n",
    "                 value =\"Viral Shedding\")\n",
    "long_biology= long_biology.replace(to_replace =\"ViralLoad\",\n",
    "                 value =\"Viral Load\")\n",
    "long_biology= long_biology.replace(to_replace =\"Mrna\",\n",
    "                 value =\"mRNA\")\n",
    "long_biology= long_biology.replace(to_replace =\"Rna\",\n",
    "                 value =\"RNA\")\n",
    "long_biology= long_biology.replace(to_replace =\"Dna\",\n",
    "                 value =\"DNA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_biology.term.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.area(long_biology, x='Day', y = 'freq', color = 'term',\n",
    "               width=970, height=2000, line_shape='spline', facet_row=\"term\",\n",
    "              template=\"ggplot2\",\n",
    "             labels={ # replaces default labels by column name\n",
    "                'term':'Term', \"Day\": \"Session date\", \"freq\": \"Frequency score\"})\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text='',  nticks=2 )\n",
    "\n",
    "fig.update_xaxes(ticklabelposition= 'inside top')\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend = False, yaxis={'visible': False, 'showticklabels': False}, font_family=\"Gill Sans\")\n",
    "\n",
    "#     yaxis=dict(\n",
    "#         autorange=True,\n",
    "#         showgrid=False,\n",
    "#         ticks='',\n",
    "#         showticklabels=False\n",
    "\n",
    "\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "\n",
    "for annotation in fig['layout']['annotations']: \n",
    "    annotation['textangle']= 0\n",
    "    annotation['yshift']=40\n",
    "    annotation['font'] = dict(\n",
    "                color=\"black\",\n",
    "                size=11)\n",
    "    annotation['x']=0.45\n",
    "\n",
    "    \n",
    "# fig.update_xaxes(rangeslider= {'visible':True}, type=\"date\", row=1, col=1)\n",
    "    \n",
    "fig.show()\n",
    "fig.write_html(\"../graphs/biology.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immunization-related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.type.unique()\n",
    "immunization = (lexicon[\"type\"] == 'immunization')\n",
    "df_immunization = lexicon.loc[immunization]\n",
    "df_immunization\n",
    "\n",
    "vocab_immunization = df_immunization['word'].tolist()\n",
    "len(vocab_immunization)\n",
    "vocab_immunization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "count_dict={}\n",
    "for v in vocab_immunization:\n",
    "    items=[]\n",
    "    x=1\n",
    "    while x<=len(trans_list_clean):\n",
    "        items.append(0)\n",
    "        x+=1\n",
    "    count_dict[v]=items\n",
    "#     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "count_dict\n",
    "\n",
    "df = pd.DataFrame(count_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trans_list_clean)):\n",
    "    words=trans_list_clean[i].split()\n",
    "    listemmed = stemmer.stem(trans_list_clean[i].lower())\n",
    "#     count=0\n",
    "#     filtered = []\n",
    "    for word in words:\n",
    "        word_stemmed = stemmer.stem(word.lower())\n",
    "        count=0\n",
    "        count = vocab_immunization.count(word.lower())\n",
    "        if count==1:\n",
    "            print(word.lower())\n",
    "            count_dict[word.lower()][i]+=1\n",
    "#         print(word_stemmed)\n",
    "#     if word_stemmed in vocabulary:\n",
    "#         filtered.append(word_stemmed)\n",
    "\n",
    "count_dict\n",
    "\n",
    "# Counter(words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "immunization = pd.DataFrame(count_dict)\n",
    "immunization\n",
    "immunization.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with zeros only\n",
    "immunization_filt=immunization.loc[(immunization.sum(axis=1) != 0), (immunization.sum(axis=0) != 0)]\n",
    "immunization_filt.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "immunization_filt=immunization_filt.drop(['takeup', ], axis = 1)\n",
    "immunization_filt.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "# sprea_filt=sprea_filt.drop(['symptom', 'symptoms', 'non-critical-covid', 'mild-covid'], axis = 1)\n",
    "\n",
    "# sprea_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "immunization_filt_norm = min_max_scaling(immunization_filt)\n",
    "\n",
    "immunization_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_string = ''.join(trans_list_clean[0])\n",
    "joined_string[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "# tokens.remove('')        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame({'day': ['4/15/2020', '4/20/2020', '4/22/2020', \n",
    "                              '4/27/2020', '5/4/2020', '5/11/2020', '5/18/2020', '6/1/2020', '6/8/2020', \n",
    "                            '6/15/2020', '6/29/2020', '7/13/2020', '7/20/2020', '7/27/2020', \n",
    "                              '8/3/2020', '8/10/2020', '8/17/2020', '8/24/2020', '8/31/2020',\n",
    "                            '9/14/2020', '9/21/2020',  '9/28/2020', '10/5/2020', '10/20/2020',\n",
    "                             '10/26/2020', '11/2/2020', '11/9/2020',\n",
    "                             '11/16/2020', '11/30/2020', '12/7/2020', \n",
    "                              '12/14/2020', '12/21/2020', '1/4/2021',\n",
    "                             '1/11/2021', '1/25/2021', '2/1/2021', \n",
    "                              '2/8/2021', '2/15/2021', '2/22/2021',\n",
    "                             '3/8/2021', '3/15/2021', '3/22/2021', \n",
    "                              '3/29/2021', '4/5/2021', '4/12/2021',\n",
    "                             '4/19/2021', '4/26/2021', '5/10/2021']})\n",
    "\n",
    "immunization_filt_norm['day'] = dates\n",
    "immunization_filt_norm['day'] = pd.to_datetime(immunization_filt_norm['day'])\n",
    "immunization_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm.to_csv('../data/clean/test.csv')\n",
    "immunization_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_low = immunization_filt_norm.columns.values.tolist()\n",
    "columns_title=[]\n",
    "for c in columns_low:\n",
    "    x=c.title()\n",
    "    columns_title.append(x)\n",
    "immunization_filt_norm.reset_index(drop=True, inplace=True)\n",
    "columns_low\n",
    "# sprea_filt_norm\n",
    "\n",
    "# columns = ['screening',\n",
    "#             'contact-tracing',\n",
    "#  'blood-test',\n",
    "#  'false-negative',\n",
    "#  'false-positive',\n",
    "#  'antibody-test',\n",
    "#  'test',\n",
    "#             'pcr',\n",
    "#  'lateral-flow-test',\n",
    "#  'pooled-testing',\n",
    "#             'rapid-test',\n",
    "#  'saliva-test',\n",
    "#  'saliva',\n",
    "# 'swab-test',\n",
    "#  'point-of-care',           \n",
    "#  's-gene',           \n",
    "#            'accuracy',\n",
    "#  'sensitivity',\n",
    "#  'specificity',    \n",
    "#  'sequencing',           \n",
    "#  'antigen',\n",
    "#  'confirmed',\n",
    "#  'suspected',\n",
    "           \n",
    "#  'quarantine',\n",
    "#  'isolation',\n",
    "#  'mask',\n",
    "#  'n95',\n",
    "#  'ppe',\n",
    "#  'fomite',\n",
    "#  'aerosol',\n",
    "#  'airborne',\n",
    "#  'droplet',\n",
    "\n",
    "\n",
    "#  'asymptomatic',\n",
    "\n",
    "# 'case-fatality',\n",
    "#  'fatality',\n",
    "#  'index-case',\n",
    "#  'epidemiology',\n",
    "# #  'epidemiologist',\n",
    "# 'outbreak',           \n",
    "#  'flatten-the-curve',\n",
    "#  'wave',           \n",
    "#  'curve',\n",
    "#  'spread',\n",
    "#  'contagion',\n",
    "#  'transmissibility',\n",
    "#  'transmission',           \n",
    "#  'containment',           \n",
    "#  'infection-fatality',           \n",
    "#  'immunity',\n",
    "#  'incidence',\n",
    "#  'mortality',\n",
    "#  'prevalence',\n",
    "#  'reproduction-number',           \n",
    "# 'incubation',\n",
    "#  'superspreader',\n",
    "#  'exposed',\n",
    "#  'infectious',\n",
    "#  'hand-washing',\n",
    "#  'hygiene',\n",
    "#  'hygienic',\n",
    "#  'social-distance',\n",
    "#  'serologic',\n",
    "#  'sanitizer',\n",
    "#  'npis',\n",
    "#  'day']\n",
    "\n",
    "immunization_filt_norm = immunization_filt_norm[columns_low]\n",
    "columns=columns_low \n",
    "\n",
    "columns.remove('day')\n",
    "# len(treatment_filt_norm)\n",
    "# all_values = []\n",
    "# for column in treatment_filt_norm:\n",
    "#     this_column_values = treatment_filt_norm[column].tolist()\n",
    "#     all_values += this_column_values\n",
    "# all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "# import numpy as np \n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# # antiviral = treatment_filt_norm['antiviral'].tolist()\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['antiviral'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#      name=\"antivirals\"    \n",
    "# )\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['plasma'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#     name=\"plasma\"\n",
    "# )\n",
    "# data = go.Data([trace0, trace1])\n",
    "\n",
    "# plotly.offline.iplot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm\n",
    "col = []\n",
    "for i in immunization_filt_norm:\n",
    "    x = 'freq_'+i.title()\n",
    "    col.append(x)\n",
    "    \n",
    "# col\n",
    "\n",
    "immunization_filt_norm.columns = col\n",
    "immunization_filt_norm\n",
    "\n",
    "\n",
    "# treatment_filt_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "immunization_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# treatment_filt_norm.info()\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Day': 'Day'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Viral-Vector': 'freq_ViralVector'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Vaccine-Coverage': 'freq_VaccineCoverage'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_T-Cell': 'freq_TCell'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Sterilizing-Immunity': 'freq_SterilizingImmunity'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Protein-Based': 'freq_ProteinBased'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Priority-Groups': 'freq_PriorityGroups'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Second-Dose': 'freq_SecondDose'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_First-Dose': 'freq_FirstDose'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Passive-Immunity': 'freq_PassiveImmunity'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Herd-Immunity': 'freq_HerdImmunity'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Mrna-Vaccine': 'freq_MrnaVaccine'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Cold-Chain': 'freq_ColdChain'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Booster-Dose': 'freq_BoosterDose'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_B-Cell': 'freq_BCell'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Attenuated-Vaccine': 'freq_AttenuatedVaccine'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Immune-System': 'freq_ImmuneSystem'}, axis=1)\n",
    "immunization_filt_norm = immunization_filt_norm.rename({'freq_Immune-Response': 'freq_ImmuneResponse'}, axis=1)\n",
    "# biology_filt_norm.reset_index(inplace=True)\n",
    "\n",
    "long= pd.wide_to_long(immunization_filt_norm, stubnames='freq', sep='_',\n",
    "                      i=['Day'], j='term', suffix=r'\\w+')\n",
    "long.reset_index(inplace=True)\n",
    "long_immunization = long.filter(['Day','term','freq'], axis=1)\n",
    "# long_biology.columns\n",
    "\n",
    "long_immunization= long_immunization.replace(to_replace =\"ViralVector\",\n",
    "                 value =\"Viral vector\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"VaccineCoverage\",\n",
    "                 value =\"Vaccine coverage\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"TCell\",\n",
    "                 value =\"T-cell\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"SterilizingImmunity\",\n",
    "                 value =\"Sterilizing immunity\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"ProteinBased\",\n",
    "                 value =\"Protein-based\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"PriorityGroups\",\n",
    "                 value =\"Priority groups\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"SecondDose\",\n",
    "                 value =\"Second dose\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"FirstDose\",\n",
    "                 value =\"First dose\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"PassiveImmunity\",\n",
    "                 value =\"Passive immunity\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"HerdImmunity\",\n",
    "                 value =\"Herd immunity\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"MrnaVaccine\",\n",
    "                 value =\"mRNA vaccine\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"ColdChain\",\n",
    "                 value =\"Cold chain\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"BoosterDose\",\n",
    "                 value =\"Booster dose\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"BCell\",\n",
    "                 value =\"B-cell\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"AttenuatedVaccine\",\n",
    "                 value =\"Attenuated vaccine\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"ImmuneSystem\",\n",
    "                 value =\"Immune system\")\n",
    "long_immunization= long_immunization.replace(to_replace =\"ImmuneResponse\",\n",
    "                 value =\"Immune response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_immunization.term.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.area(long_immunization, x='Day', y = 'freq', color = 'term',\n",
    "               width=970, height=2500, line_shape='spline', facet_row=\"term\",\n",
    "              template=\"ggplot2\",\n",
    "             labels={ # replaces default labels by column name\n",
    "                'term':'Term', \"Day\": \"Session date\", \"freq\": \"Frequency score\"},facet_row_spacing = 0.01)\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text='',  nticks=2 )\n",
    "\n",
    "fig.update_xaxes(ticklabelposition= 'inside top')\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend = False, yaxis={'visible': False, 'showticklabels': False}, font_family=\"Gill Sans\")\n",
    "\n",
    "#     yaxis=dict(\n",
    "#         autorange=True,\n",
    "#         showgrid=False,\n",
    "#         ticks='',\n",
    "#         showticklabels=False\n",
    "\n",
    "\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "\n",
    "for annotation in fig['layout']['annotations']: \n",
    "    annotation['textangle']= 0\n",
    "    annotation['yshift']=35\n",
    "    annotation['font'] = dict(\n",
    "                color=\"black\",\n",
    "                size=11)\n",
    "    annotation['x']=0.45\n",
    "\n",
    "    \n",
    "# fig.update_xaxes(rangeslider= {'visible':True}, type=\"date\", row=1, col=1)\n",
    "    \n",
    "fig.show()\n",
    "fig.write_html(\"../graphs/immunization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research and ststistics-related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.type.unique()\n",
    "research = (lexicon[\"type\"] == 'statistics') | (lexicon[\"type\"] == 'research' )\n",
    "df_research = lexicon.loc[research]\n",
    "df_research\n",
    "\n",
    "vocab_research = df_research['word'].tolist()\n",
    "len(vocab_research)\n",
    "vocab_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "count_dict={}\n",
    "for v in vocab_research:\n",
    "    items=[]\n",
    "    x=1\n",
    "    while x<=len(trans_list_clean):\n",
    "        items.append(0)\n",
    "        x+=1\n",
    "    count_dict[v]=items\n",
    "#     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "count_dict\n",
    "\n",
    "df = pd.DataFrame(count_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trans_list_clean)):\n",
    "    words=trans_list_clean[i].split()\n",
    "    listemmed = stemmer.stem(trans_list_clean[i].lower())\n",
    "#     count=0\n",
    "#     filtered = []\n",
    "    for word in words:\n",
    "        word_stemmed = stemmer.stem(word.lower())\n",
    "        count=0\n",
    "        count = vocab_research.count(word.lower())\n",
    "        if count==1:\n",
    "            print(word.lower())\n",
    "            count_dict[word.lower()][i]+=1\n",
    "#         print(word_stemmed)\n",
    "#     if word_stemmed in vocabulary:\n",
    "#         filtered.append(word_stemmed)\n",
    "\n",
    "count_dict\n",
    "\n",
    "# Counter(words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "research = pd.DataFrame(count_dict)\n",
    "research\n",
    "research.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with zeros only\n",
    "research_filt=research.loc[(research.sum(axis=1) != 0), (research.sum(axis=0) != 0)]\n",
    "research_filt.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "# research_filt=research_filt.drop(['takeup', ], axis = 1)\n",
    "research_filt.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "# sprea_filt=sprea_filt.drop(['symptom', 'symptoms', 'non-critical-covid', 'mild-covid'], axis = 1)\n",
    "\n",
    "# sprea_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "research_filt_norm = min_max_scaling(research_filt)\n",
    "\n",
    "research_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_string = ''.join(trans_list_clean[0])\n",
    "joined_string[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "# tokens.remove('')        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame({'day': ['4/15/2020', '4/20/2020', '4/22/2020', \n",
    "                              '4/27/2020', '5/4/2020', '5/11/2020', '5/18/2020', '6/1/2020', '6/8/2020', \n",
    "                            '6/15/2020', '6/29/2020', '7/13/2020', '7/20/2020', '7/27/2020', \n",
    "                              '8/3/2020', '8/10/2020', '8/17/2020', '8/24/2020', '8/31/2020',\n",
    "                            '9/14/2020', '9/21/2020',  '9/28/2020', '10/5/2020', '10/20/2020',\n",
    "                             '10/26/2020', '11/2/2020', '11/9/2020',\n",
    "                             '11/16/2020', '11/30/2020', '12/7/2020', \n",
    "                              '12/14/2020', '12/21/2020', '1/4/2021',\n",
    "                             '1/11/2021', '1/25/2021', '2/1/2021', \n",
    "                              '2/8/2021', '2/15/2021', '2/22/2021',\n",
    "                             '3/8/2021', '3/15/2021', '3/22/2021', \n",
    "                              '3/29/2021', '4/5/2021', '4/12/2021',\n",
    "                             '4/19/2021', '4/26/2021', '5/10/2021']})\n",
    "\n",
    "research_filt_norm['day'] = dates\n",
    "research_filt_norm['day'] = pd.to_datetime(research_filt_norm['day'])\n",
    "research_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm.to_csv('../data/clean/test.csv')\n",
    "research_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_low = research_filt_norm.columns.values.tolist()\n",
    "columns_title=[]\n",
    "for c in columns_low:\n",
    "    x=c.title()\n",
    "    columns_title.append(x)\n",
    "research_filt_norm.reset_index(drop=True, inplace=True)\n",
    "columns_low\n",
    "# sprea_filt_norm\n",
    "\n",
    "# columns = ['screening',\n",
    "#             'contact-tracing',\n",
    "#  'blood-test',\n",
    "#  'false-negative',\n",
    "#  'false-positive',\n",
    "#  'antibody-test',\n",
    "#  'test',\n",
    "#             'pcr',\n",
    "#  'lateral-flow-test',\n",
    "#  'pooled-testing',\n",
    "#             'rapid-test',\n",
    "#  'saliva-test',\n",
    "#  'saliva',\n",
    "# 'swab-test',\n",
    "#  'point-of-care',           \n",
    "#  's-gene',           \n",
    "#            'accuracy',\n",
    "#  'sensitivity',\n",
    "#  'specificity',    \n",
    "#  'sequencing',           \n",
    "#  'antigen',\n",
    "#  'confirmed',\n",
    "#  'suspected',\n",
    "           \n",
    "#  'quarantine',\n",
    "#  'isolation',\n",
    "#  'mask',\n",
    "#  'n95',\n",
    "#  'ppe',\n",
    "#  'fomite',\n",
    "#  'aerosol',\n",
    "#  'airborne',\n",
    "#  'droplet',\n",
    "\n",
    "\n",
    "#  'asymptomatic',\n",
    "\n",
    "# 'case-fatality',\n",
    "#  'fatality',\n",
    "#  'index-case',\n",
    "#  'epidemiology',\n",
    "# #  'epidemiologist',\n",
    "# 'outbreak',           \n",
    "#  'flatten-the-curve',\n",
    "#  'wave',           \n",
    "#  'curve',\n",
    "#  'spread',\n",
    "#  'contagion',\n",
    "#  'transmissibility',\n",
    "#  'transmission',           \n",
    "#  'containment',           \n",
    "#  'infection-fatality',           \n",
    "#  'immunity',\n",
    "#  'incidence',\n",
    "#  'mortality',\n",
    "#  'prevalence',\n",
    "#  'reproduction-number',           \n",
    "# 'incubation',\n",
    "#  'superspreader',\n",
    "#  'exposed',\n",
    "#  'infectious',\n",
    "#  'hand-washing',\n",
    "#  'hygiene',\n",
    "#  'hygienic',\n",
    "#  'social-distance',\n",
    "#  'serologic',\n",
    "#  'sanitizer',\n",
    "#  'npis',\n",
    "#  'day']\n",
    "\n",
    "research_filt_norm = research_filt_norm[columns_low]\n",
    "columns=columns_low \n",
    "\n",
    "columns.remove('day')\n",
    "# len(treatment_filt_norm)\n",
    "# all_values = []\n",
    "# for column in treatment_filt_norm:\n",
    "#     this_column_values = treatment_filt_norm[column].tolist()\n",
    "#     all_values += this_column_values\n",
    "# all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "# import numpy as np \n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# # antiviral = treatment_filt_norm['antiviral'].tolist()\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['antiviral'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#      name=\"antivirals\"    \n",
    "# )\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['plasma'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#     name=\"plasma\"\n",
    "# )\n",
    "# data = go.Data([trace0, trace1])\n",
    "\n",
    "# plotly.offline.iplot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm\n",
    "col = []\n",
    "for i in research_filt_norm:\n",
    "    x = 'freq_'+i.title()\n",
    "    col.append(x)\n",
    "    \n",
    "# col\n",
    "\n",
    "research_filt_norm.columns = col\n",
    "research_filt_norm\n",
    "\n",
    "\n",
    "# treatment_filt_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "research_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# treatment_filt_norm.info()\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Day': 'Day'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Clinical-Trial': 'freq_ClinicalTrial'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Phase-1': 'freq_Phase1'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Phase2': 'freq_Phase2'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Phase-3': 'freq_Phase3'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Randomized-Controlled-Trial': 'freq_RandomizedControlledTrial'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Peer-Review': 'freq_PeerReview'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_New-England-Journal': 'freq_NewEnglandJournal'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Odds-Ratio': 'freq_OddsRatio'}, axis=1)\n",
    "research_filt_norm = research_filt_norm.rename({'freq_Relative-Risk': 'freq_RelativeRisk'}, axis=1)\n",
    "\n",
    "long= pd.wide_to_long(research_filt_norm, stubnames='freq', sep='_',\n",
    "                      i=['Day'], j='term', suffix=r'\\w+')\n",
    "long.reset_index(inplace=True)\n",
    "long_research = long.filter(['Day','term','freq'], axis=1)\n",
    "# long_biology.columns\n",
    "\n",
    "long_research= long_research.replace(to_replace =\"ClinicalTrial\",\n",
    "                 value =\"Clinical trial\")\n",
    "long_research= long_research.replace(to_replace =\"Phase1\",\n",
    "                 value =\"Phase 1\")\n",
    "long_research= long_research.replace(to_replace =\"Phase2\",\n",
    "                 value =\"Phase 2\")\n",
    "long_research= long_research.replace(to_replace =\"Phase3\",\n",
    "                 value =\"Phase 3\")\n",
    "long_research= long_research.replace(to_replace =\"RandomizedControlledTrial\",\n",
    "                 value =\"Randomized controlled trial\")\n",
    "long_research= long_research.replace(to_replace =\"PeerReview\",\n",
    "                 value =\"Peer review\")\n",
    "long_research= long_research.replace(to_replace =\"NewEnglandJournal\",\n",
    "                 value =\"New England Journal\")\n",
    "long_research= long_research.replace(to_replace =\"OddsRatio\",\n",
    "                 value =\"Odds ratio\")\n",
    "long_research= long_research.replace(to_replace =\"RelativeRisk\",\n",
    "                 value =\"Relative risk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_research.term.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode() \n",
    "\n",
    "fig = px.area(long_research, x='Day', y = 'freq', color = 'term',\n",
    "               width=970, height=2000, line_shape='spline', facet_row=\"term\",\n",
    "              template=\"ggplot2\",\n",
    "             labels={ # replaces default labels by column name\n",
    "                'term':'Term', \"Day\": \"Session date\", \"freq\": \"Frequency score\"})\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text='',  nticks=2 )\n",
    "\n",
    "fig.update_xaxes(ticklabelposition= 'inside top')\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend = False, yaxis={'visible': False, 'showticklabels': False}, font_family=\"Gill Sans\")\n",
    "\n",
    "#     yaxis=dict(\n",
    "#         autorange=True,\n",
    "#         showgrid=False,\n",
    "#         ticks='',\n",
    "#         showticklabels=False\n",
    "\n",
    "\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "\n",
    "for annotation in fig['layout']['annotations']: \n",
    "    annotation['textangle']= 0\n",
    "    annotation['yshift']=40\n",
    "    annotation['font'] = dict(\n",
    "                color=\"black\",\n",
    "                size=11)\n",
    "    annotation['x']=0.45\n",
    "\n",
    "    \n",
    "# fig.update_xaxes(rangeslider= {'visible':True}, type=\"date\", row=1, col=1)\n",
    "    \n",
    "fig.show()\n",
    "fig.write_html(\"../graphs/research.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.type.unique()\n",
    "risk =  (lexicon[\"type\"] == 'determinants' )\n",
    "df_risk = lexicon.loc[risk]\n",
    "df_risk\n",
    "\n",
    "vocab_risk = df_risk['word'].tolist()\n",
    "len(vocab_risk)\n",
    "vocab_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "count_dict={}\n",
    "for v in vocab_risk:\n",
    "    items=[]\n",
    "    x=1\n",
    "    while x<=len(trans_list_clean):\n",
    "        items.append(0)\n",
    "        x+=1\n",
    "    count_dict[v]=items\n",
    "#     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "count_dict\n",
    "\n",
    "df = pd.DataFrame(count_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trans_list_clean)):\n",
    "    words=trans_list_clean[i].split()\n",
    "    listemmed = stemmer.stem(trans_list_clean[i].lower())\n",
    "#     count=0\n",
    "#     filtered = []\n",
    "    for word in words:\n",
    "        word_stemmed = stemmer.stem(word.lower())\n",
    "        count=0\n",
    "        count = vocab_risk.count(word.lower())\n",
    "        if count==1:\n",
    "            print(word.lower())\n",
    "            count_dict[word.lower()][i]+=1\n",
    "#         print(word_stemmed)\n",
    "#     if word_stemmed in vocabulary:\n",
    "#         filtered.append(word_stemmed)\n",
    "\n",
    "count_dict\n",
    "\n",
    "# Counter(words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "risk = pd.DataFrame(count_dict)\n",
    "risk\n",
    "risk.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with zeros only\n",
    "risk_filt=risk.loc[(risk.sum(axis=1) != 0), (risk.sum(axis=0) != 0)]\n",
    "risk_filt.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "# research_filt=research_filt.drop(['takeup', ], axis = 1)\n",
    "risk_filt.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns that aren't relevant\n",
    "# sprea_filt=sprea_filt.drop(['symptom', 'symptoms', 'non-critical-covid', 'mild-covid'], axis = 1)\n",
    "\n",
    "# sprea_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "\n",
    "risk_filt_norm = min_max_scaling(risk_filt)\n",
    "risk_filt_norm\n",
    "# risk_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_string = ''.join(trans_list_clean[0])\n",
    "joined_string[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "# tokens.remove('')        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame({'day': ['4/15/2020', '4/20/2020', '4/22/2020', \n",
    "                              '4/27/2020', '5/4/2020', '5/11/2020', '5/18/2020', '6/1/2020', '6/8/2020', \n",
    "                            '6/15/2020', '6/29/2020', '7/13/2020', '7/20/2020', '7/27/2020', \n",
    "                              '8/3/2020', '8/10/2020', '8/17/2020', '8/24/2020', '8/31/2020',\n",
    "                            '9/14/2020', '9/21/2020',  '9/28/2020', '10/5/2020', '10/20/2020',\n",
    "                             '10/26/2020', '11/2/2020', '11/9/2020',\n",
    "                             '11/16/2020', '11/30/2020', '12/7/2020', \n",
    "                              '12/14/2020', '12/21/2020', '1/4/2021',\n",
    "                             '1/11/2021', '1/25/2021', '2/1/2021', \n",
    "                              '2/8/2021', '2/15/2021', '2/22/2021',\n",
    "                             '3/8/2021', '3/15/2021', '3/22/2021', \n",
    "                              '3/29/2021', '4/5/2021', '4/12/2021',\n",
    "                             '4/19/2021', '4/26/2021', '5/10/2021']})\n",
    "\n",
    "risk_filt_norm['day'] = dates\n",
    "risk_filt_norm['day'] = pd.to_datetime(risk_filt_norm['day'])\n",
    "risk_filt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm.to_csv('../data/clean/test.csv')\n",
    "risk_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_low = risk_filt_norm.columns.values.tolist()\n",
    "columns_title=[]\n",
    "for c in columns_low:\n",
    "    x=c.title()\n",
    "    columns_title.append(x)\n",
    "risk_filt_norm.reset_index(drop=True, inplace=True)\n",
    "columns_low\n",
    "# sprea_filt_norm\n",
    "\n",
    "# columns = ['screening',\n",
    "#             'contact-tracing',\n",
    "#  'blood-test',\n",
    "#  'false-negative',\n",
    "#  'false-positive',\n",
    "#  'antibody-test',\n",
    "#  'test',\n",
    "#             'pcr',\n",
    "#  'lateral-flow-test',\n",
    "#  'pooled-testing',\n",
    "#             'rapid-test',\n",
    "#  'saliva-test',\n",
    "#  'saliva',\n",
    "# 'swab-test',\n",
    "#  'point-of-care',           \n",
    "#  's-gene',           \n",
    "#            'accuracy',\n",
    "#  'sensitivity',\n",
    "#  'specificity',    \n",
    "#  'sequencing',           \n",
    "#  'antigen',\n",
    "#  'confirmed',\n",
    "#  'suspected',\n",
    "           \n",
    "#  'quarantine',\n",
    "#  'isolation',\n",
    "#  'mask',\n",
    "#  'n95',\n",
    "#  'ppe',\n",
    "#  'fomite',\n",
    "#  'aerosol',\n",
    "#  'airborne',\n",
    "#  'droplet',\n",
    "\n",
    "\n",
    "#  'asymptomatic',\n",
    "\n",
    "# 'case-fatality',\n",
    "#  'fatality',\n",
    "#  'index-case',\n",
    "#  'epidemiology',\n",
    "# #  'epidemiologist',\n",
    "# 'outbreak',           \n",
    "#  'flatten-the-curve',\n",
    "#  'wave',           \n",
    "#  'curve',\n",
    "#  'spread',\n",
    "#  'contagion',\n",
    "#  'transmissibility',\n",
    "#  'transmission',           \n",
    "#  'containment',           \n",
    "#  'infection-fatality',           \n",
    "#  'immunity',\n",
    "#  'incidence',\n",
    "#  'mortality',\n",
    "#  'prevalence',\n",
    "#  'reproduction-number',           \n",
    "# 'incubation',\n",
    "#  'superspreader',\n",
    "#  'exposed',\n",
    "#  'infectious',\n",
    "#  'hand-washing',\n",
    "#  'hygiene',\n",
    "#  'hygienic',\n",
    "#  'social-distance',\n",
    "#  'serologic',\n",
    "#  'sanitizer',\n",
    "#  'npis',\n",
    "#  'day']\n",
    "\n",
    "risk_filt_norm = risk_filt_norm[columns_low]\n",
    "columns=columns_low \n",
    "\n",
    "columns.remove('day')\n",
    "# len(treatment_filt_norm)\n",
    "# all_values = []\n",
    "# for column in treatment_filt_norm:\n",
    "#     this_column_values = treatment_filt_norm[column].tolist()\n",
    "#     all_values += this_column_values\n",
    "# all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "# import numpy as np \n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# # antiviral = treatment_filt_norm['antiviral'].tolist()\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['antiviral'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#      name=\"antivirals\"    \n",
    "# )\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#    x = treatment_filt_norm['day'],\n",
    "#     y = treatment_filt_norm['plasma'],\n",
    "#     fill='tozeroy',\n",
    "#     line_shape='spline',\n",
    "#     name=\"plasma\"\n",
    "# )\n",
    "# data = go.Data([trace0, trace1])\n",
    "\n",
    "# plotly.offline.iplot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment_filt_norm\n",
    "col = []\n",
    "for i in risk_filt_norm:\n",
    "    x = 'freq_'+i.title()\n",
    "    col.append(x)\n",
    "    \n",
    "# col\n",
    "\n",
    "risk_filt_norm.columns = col\n",
    "risk_filt_norm\n",
    "\n",
    "\n",
    "# treatment_filt_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "risk_filt_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "risk_filt_norm.info()\n",
    "risk_filt_norm = risk_filt_norm.rename({'freq_Day': 'Day'}, axis=1)\n",
    "risk_filt_norm = risk_filt_norm.rename({'freq_Native-American': 'freq_NativeAmerican'}, axis=1)\n",
    "\n",
    "risk_filt_norm\n",
    "long= pd.wide_to_long(risk_filt_norm, stubnames='freq', sep='_',\n",
    "                      i=['Day'], j='term', suffix=r'\\w+')\n",
    "long.reset_index(inplace=True)\n",
    "long_risk = long.filter(['Day','term','freq'], axis=1)\n",
    "\n",
    "long_risk= long_risk.replace(to_replace =\"NativeAmerican\",\n",
    "                 value =\"Native American\")\n",
    "long_risk= long_risk.replace(to_replace =\"Bmi\",\n",
    "                 value =\"BMI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_risk.term.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode() \n",
    "\n",
    "fig = px.area(long_risk, x='Day', y = 'freq', color = 'term',\n",
    "               width=970, height=1200, line_shape='spline', facet_row=\"term\",\n",
    "              template=\"ggplot2\",\n",
    "             labels={ # replaces default labels by column name\n",
    "                'term':'Term', \"Day\": \"Session date\", \"freq\": \"Frequency score\"})\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text='',  nticks=2 )\n",
    "\n",
    "fig.update_xaxes(ticklabelposition= 'inside top')\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend = False, yaxis={'visible': False, 'showticklabels': False}, font_family=\"Gill Sans\",)\n",
    "\n",
    "#     yaxis=dict(\n",
    "#         autorange=True,\n",
    "#         showgrid=False,\n",
    "#         ticks='',\n",
    "#         showticklabels=False\n",
    "\n",
    "\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "\n",
    "for annotation in fig['layout']['annotations']: \n",
    "    annotation['textangle']= 0\n",
    "    annotation['yshift']=40\n",
    "    annotation['font'] = dict(\n",
    "                color=\"black\",\n",
    "                size=11)\n",
    "    annotation['x']=0.45\n",
    "\n",
    "    \n",
    "# fig.update_xaxes(rangeslider= {'visible':True}, type=\"date\", row=1, col=1)\n",
    "    \n",
    "fig.show()\n",
    "fig.write_html(\"../graphs/risk.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
